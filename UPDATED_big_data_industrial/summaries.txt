

 ************ Summaries for file before_CSCC-Cloud-Customer-Architecture-for-Big-Data-and-Analytics.txt. ************ 

--------- Summary by usual TextRank algorithm --------- 
The choice of cloud architectures allows compute components to be moved near data to optimize  processing when data volume and bandwidth limitations produce remote data bottlenecks.The amount of data that the analytics processing system needs would then determine whether the  analytics needs to be hosted with this data, or whether it can use APIs to retrieve the data it needs remotely.Critical data and processing remains in the enterprise data center, while other  resources are deployed in public cloud environments.The provider cloud uses data integration components and potentially streaming computing to capture this combined data into data repositories where analytics can be performed to deliver actionable  insights.The data is collected from structured and non-structured data sources, including real-time data from  stream computing, and maintained in enterprise data stores.Data sources contain all of the external sources of data for the data analytics solutions that flow from  the internet.When the data or user requests comes from the external internet, the flow may come through edge  services including DNS servers, Content Delivery Networks (CDNs), firewalls, and load balancers before  entering the cloud provider’s data integration or data streaming entry points.Data Sources There can be a number of different information sources in a typical big data system, some of which enterprises are just beginning to include in their data  analytics solutions.
--------- Summary by LSA algorithm --------- 
Using analytics reveals patterns, trends and associations in data that help an organization understand the behavior of the people and systems that drive its operation.Typically  phase one and two (the discovery, exploration and analytical model development) is collated with a vast  collection of different types of data that has been harvested from its original sources.Preliminary and  proof-of-concept (POC) applications often start in public cloud environments where new resources can  be acquired and evaluated quickly with a minimal procurement process.Another benefit is the ability to develop applications on dedicated resource pools in a hybrid cloud  deployment that eliminates the need to compromise on configuration details like processors, GPUs,  memory, networking and even software licensing constraints.Once the models are executed, their outcome is provided  for use as actionable insight via information views into the data which are also exposed for ad-hoc  analysis by end users or other applications on the client premises.Information governance and security subsystems encompass each processing phase to ensure regulation and policies for all data are defined and enabled across the system.Transformation and connectivity gateways assist by  preparing information for use by enterprise applications as well as use on different devices, including  mobile, web browsers and desktop systems.Cloud computing  helps mitigate that investment and the associated risk by providing big data  tools via a pay per use model.
--------- Summary by Kullback–Leibler algorithm --------- 
Big data technology increases the  amount and variety of data that can be processed by analytics, providing a foundation for visualizations  and insights that can significantly improve business operations.In particular, where should the data be located and where  should the analytics processing be located relative to the location of the data?Legal and regulatory  requirements may also impact where data can be located since many countries have data sovereignty  laws that prevent data about individuals, finances and intellectual property from moving across country  borders.The amount of data that the analytics processing system needs would then determine whether the  analytics needs to be hosted with this data, or whether it can use APIs to retrieve the data it needs remotely.Organizations needing on-premises data storage and processing cite data privacy, security and legal  constraints as chief motivations.Data sources contain all of the external sources of data for the data analytics solutions that flow from  the internet.Data Sources There can be a number of different information sources in a typical big data system, some of which enterprises are just beginning to include in their data  analytics solutions.Big data tools  have enabled organizations to use this data; however, these typically run onpremises and can require substantial upfront investment.
--------- Summary by LexRank algorithm --------- 
The architectural  elements described in this document will help you understand the components for leveraging various cloud deployment models.Cloud deployments offer a choice of private, public and hybrid architectures.It would be expensive to  move, so the analytics processing system may need to access this data from its current storage location.Each phase may run in the same cloud environment or be distributed in different locations.Wherever the analytics model is  deployed, it is accompanied by new data collection processes that gather the results of the analytics so  they can be improved with another iteration of the analytics development lifecycle.The data is collected from structured and non-structured data sources, including real-time data from  stream computing, and maintained in enterprise data stores.The data repositories provide the development environment for new analytics  models or enhancements of existing models.Data Sources There can be a number of different information sources in a typical big data system, some of which enterprises are just beginning to include in their data  analytics solutions.

 ************ Summaries for file before_Big_Data_Analytics_for_Security_Intelligence.txt. ************ 

--------- Summary by usual TextRank algorithm --------- 
In particular, Big Data analytics can be  leveraged  to  improve  information  security  and  situational  awareness.For  example,  Big  Data  analytics  can  be  employed  to  analyze  financial  transactions,  log  files,  and  network  traffic  to  identify  anomalies  and  suspicious  activities, and to correlate multiple sources of information into a coherent view.Fraud  detection  is  one  of  the  most  visible  uses  for  Big  Data  analytics.However,  the  custom-built  infrastructure  to  mine  Big  Data  for  fraud  detection was not economical to adapt for other fraud detection uses.For  example,  Big  Data  analytics  can  be  employed  to  analyze  financial  transactions,  log  files,  and  network  traffic  to  identify  anomalies  and  suspicious  activities, and to correlate multiple sources of information into a coherent view.Fraud  detection  is  one  of  the  most  visible  uses  for  Big  Data  analytics.However,  the  custom-built  infrastructure  to  mine  Big  Data  for  fraud  detection was not economical to adapt for other fraud detection uses.Experiments on a 2 billion HTTP request data  set collected at a large enterprise, a 1 billion DNS request data set collected at an ISP, and a 35 billion network  intrusion detection system alert data set collected from over 900 enterprises worldwide showed that high true  positive rates and low false positive rates can be achieved with minimal ground truth information (that is, having  limited data labeled as normal events or attack events used to train anomaly detectors).
--------- Summary by LSA algorithm --------- 
For  example,  Big  Data  analytics  can  be  employed  to  analyze  financial  transactions,  log  files,  and  network  traffic  to  identify  anomalies  and  suspicious  activities, and to correlate multiple sources of information into a coherent view.For  example,  Big  Data  analytics  can  be  employed  to  analyze  financial  transactions,  log  files,  and  network  traffic  to  identify  anomalies  and  suspicious  activities, and to correlate multiple sources of information into a coherent view.To do so, algorithms and  systems  must  be  designed  and  implemented  in  order  to  identify  actionable  security  information  from  large  enterprise data sets and drive false positive rates down to manageable levels.At RSA Labs, the observation about APTs is that, however subtle the attack might be, the attacker’s behavior (in  attempting to steal sensitive information or subvert system operations) should cause the compromised user’s  actions  to  deviate  from  their  usual  pattern.The  human  analyst  is  given  the  flexibility  of  combining  multiple  sensors  according  to  known  attack  patterns  (e.g.,  command-and-control  communications  followed  by  lateral  movement) to look for abnormal events that may warrant investigation or to generate behavioral reports of a  given user’s activities across time.Preliminary results showed that Beehive  is able to process a day’s worth of  data (around a billion log messages) in an hour and identified policy violations and malware infections that would  otherwise have gone unnoticed (Yen et al., 2013).By using a MapReduce implementation, an APT  detection system has the possibility to more efficiently handle highly unstructured data with arbitrary formats  that are captured by many types of sensors (e.g., Syslog, IDS, Firewall, NetFlow, and DNS) over long periods of  time.This operational model also allows Symantec to record metadata establishing the provenance of experimental  results (Dumitras & Efstathopoulos, 2012), which ensures the reproducibility of past experiments conducted on  WINE.
--------- Summary by Kullback–Leibler algorithm --------- 
Big Data Analytics for  Security Intelligence.The rate of data creation has increased so much  that 90% of the data in the world today has been created in the last two years alone.As a result, the more data that is collected, the less actionable information is derived  from the data.Experiments on a 2 billion HTTP request data  set collected at a large enterprise, a 1 billion DNS request data set collected at an ISP, and a 35 billion network  intrusion detection system alert data set collected from over 900 enterprises worldwide showed that high true  positive rates and low false positive rates can be achieved with minimal ground truth information (that is, having  limited data labeled as normal events or attack events used to train anomaly detectors).A common goal of  an APT is to steal intellectual property (IP) from the targeted organization, to gain access to sensitive customer  data, or to access strategic business information that could be used for financial gain, blackmail, embarrassment,  data poisoning, illegal insider trading or disrupting an organization’s business.This massive volume of data makes the detection task look like searching for a  needle in a haystack (Giura & Wang, 2012).For instance, a sensor may keep track of  the  external  sites  a  host  contacts  in  order  to  identify  unusual  connections  (potential  command-and-control  channels), profile the set of machines each user logs into to find anomalous access patterns (potential “pivoting”  behavior in the lateral movement stage), study users’ regular working hours to flag suspicious activities in the  middle of the night, or track the flow of data between internal hosts to find unusual “sinks” where large amounts  of data are gathered (potential staging servers before data exfiltration).An attack pyramid should have the possible attack goal (e.g., sensitive data, high rank employees, and data  servers) at the top and lateral planes representing the environments where the events associated with an attack  can be recorded (e.g., user plane, network plane, application plane, or physical plane).
--------- Summary by LexRank algorithm --------- 
These  advances  have  created  several  differences  between  traditional  analytics and Big Data analytics.First,  a  large-scale  graph  inference  approach  was  introduced to identify malware-infected hosts in an enterprise network and the malicious domains accessed by  the enterprise's hosts.Experiments on a 2 billion HTTP request data  set collected at a large enterprise, a 1 billion DNS request data set collected at an ISP, and a 35 billion network  intrusion detection system alert data set collected from over 900 enterprises worldwide showed that high true  positive rates and low false positive rates can be achieved with minimal ground truth information (that is, having  limited data labeled as normal events or attack events used to train anomaly detectors).Processing  this  data  with  traditional  tools  is  challenging.APTs are operated by highly-skilled,  well-funded and motivated attackers targeting sensitive information from specific organizations and operating  over  periods  of  months  or  years.In order to protect the sensitive information included in the data sets, WINE can only be  accessed  on-site  at  Symantec  Research  Labs.To keep the data  sets up-to-date and to make them easier to analyze, WINE stores a representative sample from each telemetry  source.The samples included in WINE contain either all of the events recorded on a host or no data from that  host at all, allowing researchers to search for correlations among events from different data sets.

 ************ Summaries for file before_Big_Data_Taxonomy.txt. ************ 

--------- Summary by usual TextRank algorithm --------- 
Structured data conforms to  a database model, which is largely characterized by the various fields that data belongs to (name, address, age and so  forth), and the data type for each field (numeric, currency, alphabetic, name, date, address).Computing paradigms on big data currently differ at the first level of  abstraction on whether the processing will be done in batch mode, or in real-time/near real-time on streaming data  (data that is constantly coming in and needs to be processed right away).The data stored can either be  in the Hadoop filesystem as unstructured data, or in a  database as structured data.Because Hadoop is designed to work  on problems whose input data is very large and cannot fit in the disk size of a single computer, the MapReduce paradigm  is designed to take computation to where the data is stored, rather than move data to where the computation occurs.Time Series Data – A vast majority of big data applications are sensitive to time, so applying scalable machine  learning algorithms to time series data becomes an important task.Spatial Data – Spatial databases hold space-related data such as maps, medical imaging data, remote sensing data,  VLSI chip layout data and so forth.Abstract/Summary Visualization – Often big data analytics require data to be processed at scale (e.g. billions of records,  terabytes of data) before any meaningful correlations can be discovered.Streaming Data – There are two complementary security problems for streaming data depending on whether the data is  public or not.
--------- Summary by LSA algorithm --------- 
The main objective of this taxonomy is to help decision makers navigate the myriad choices in compute and storage infrastructures as well as data analytics techniques, and security and privacy frameworks.Some of the many applications that involve data arriving “in real-time” include the following:  On-line ad optimization (including real-time bidding), High frequency online trading platforms,  Security event monitoring, Financial transaction monitoring and fraud detection, Web analytics and other kinds of dashboards, Churn prediction for online games or e-commerce, Optimizing devices, industrial plants or logistics systems based on behavior and usage, Control systems related tasks; e.g., the SmartGrid, nuclear plants,  Sentiment analysis of tweets pertaining to a topic.On the other hand, if a process  control system is being driven by data from a  set of sensors, or an enterprise is deploying a new landing page for a  website and the objective is to detect any sudden drop in the number of visitors, a more immediate response is needed.These frameworks take care of the scaling onto multiple cluster nodes and come with  varying degrees of support for resilience and fault tolerance, for example, through checkpointing, to make sure the  system can recover from failure.To run programs faster, Spark provides primitives for  in-memory cluster computing: a job can load data into memory and query it repeatedly much more quickly than with  disk-based systems such as Hadoop MapReduce.Given these factors, many solutions have been created that provide  the scale and speed developers need when they build social, analytics, gaming, financial or medical apps with large  datasets.SSDs have higher densities and lower per-gigabyte  costs than RAM, so it should be possible to scale much larger datasets on fewer nodes for SSD databases – valuable  when talking about very large amounts of data.There are six broad categories in which this data variety can be categorized, and  the machine learning algorithms mentioned in the previous section can be adapted and/or strengthened in various ways  in order to apply to these various types of unstructured datasets.
--------- Summary by Kullback–Leibler algorithm --------- 
In addition to latency requirements (the  time available to compute the results), these could include the following:  Event Characteristics, including input/output data rate required by the application, Event Response Complexity.As might be expected, event response complexity that is high in both the compute and data domain aspects, when  coupled with high input/output data rates and low latency requirements poses the most severe challenges on the  underlying infrastructure.Because Hadoop is designed to work  on problems whose input data is very large and cannot fit in the disk size of a single computer, the MapReduce paradigm  is designed to take computation to where the data is stored, rather than move data to where the computation occurs.This class of  algorithms includes many types of machine learning algorithms that are critical for sophisticated data analytics, such as  online learning algorithms.Spark was initially developed for two  applications where keeping data in memory helps: iterative machine learning algorithms and interactive data mining.This class of algorithms is specifically interesting  for the big data community as the invariable presence of large unlabeled datasets is not well exploited by traditional  supervised learning algorithms.Time Series Data – A vast majority of big data applications are sensitive to time, so applying scalable machine  learning algorithms to time series data becomes an important task.Abstract/Summary Visualization – Often big data analytics require data to be processed at scale (e.g. billions of records,  terabytes of data) before any meaningful correlations can be discovered.
--------- Summary by LexRank algorithm --------- 
In addition to latency requirements (the  time available to compute the results), these could include the following:  Event Characteristics, including input/output data rate required by the application, Event Response Complexity.Latency is the time available between when an input event occurs and the response to that event is needed.While the specific  latencies are a function of the application and infrastructure, and will evolve over time, applications that we  include in this category are those requiring a “real-time” response.Computing paradigms on big data currently differ at the first level of  abstraction on whether the processing will be done in batch mode, or in real-time/near real-time on streaming data  (data that is constantly coming in and needs to be processed right away).This class of  algorithms includes many types of machine learning algorithms that are critical for sophisticated data analytics, such as  online learning algorithms.The Spark architecture allows this seamless combination of streaming and batch processing in one system.Time Series Data – A vast majority of big data applications are sensitive to time, so applying scalable machine  learning algorithms to time series data becomes an important task.Most machine learning algorithms require batch  processing of data such as clustering that needs to look at whole data in one pass to learn meaningful clusters.

 ************ Summaries for file before_Big-Data-New-Concerns.txt. ************ 

--------- Summary by usual TextRank algorithm --------- 
The Report identifies five  areas of focus:  protecting privacy, preventing discrimination, ensuring responsible use of information by  government agencies, harnessing data as a public resource, and using big data to enhance learning  opportunities.The Report concludes with policy recommendations, including, advancing the Consumer Privacy  Bill of Rights; passing national data breach legislation; amending the Electronic Communications Privacy Act;  expanding technical expertise to stop discrimination; extending privacy protections to non-U.S. persons; and  ensuring that data collected on students in school are used for educational purposes.Federal agencies with expertise in  privacy and data practices should provide technical assistance to state, local, and other federal law enforcement  agencies seeking to deploy big data techniques.The Report recommends the adoption of a national data privacy law that would incorporate the principles laid  out in the White House Consumer Privacy Bill of Rights.The Report recognizes that notice and consent  would be incompatible with the way big data functions, because it would block new, non-obvious, unexpectedly  powerful uses of data.The Report encourages the  data broker industry to build a portal where data brokers would disclose their data practices and provide  methods for consumers to better control the collection and use of their information and to opt-out of certain  marketing uses.The Report recommends that the 1974 Privacy  Act be applied to non-U.S. persons where practicable, or that alternative privacy policies that provide  appropriate and meaningful protection be applied to personal information regardless of a person’s nationality.However, big data analytics and technologies - especially when combined with the new means of collecting  personal information such as sensors, wearable technologies, smart grid, or Internet of things devices - create  the potential for new uses of data.
--------- Summary by LSA algorithm --------- 
Large amounts of data are being processed through new techniques and technologies,  dissecting the digital footprints individuals leave behind, and revealing a surprising number of personal details.As a result, big data analytics have the potential to eclipse longstanding civil rights protections in how personal  information is used in housing, credit, employment, health, education, and the marketplace.The general theme of the Report and its recommendations  center on finding responsible uses of big data for the benefit of individuals, respecting privacy and intimacy, and  setting up better structures, disclosures or technologies to allow for these new uses.There is no doubt that the Electronic Communications Privacy Act, almost 30 years old, is out of sync with the  reality of today’s cloud services, texting, social media and other means that did not exist or were in their infancy  in 1986.Consistent with the numerous initiatives already in progress, the Report recommends amending the  ECPA to provide the same protection for online, digital content as that which is afforded in the physical world.Data brokers have been the subject of intense scrutiny in the past few years, including several initiatives by the  Federal Trade Commission, alleging violation of the U.S. Fair Credit Reporting Act.Departments of State and Commerce to engage with the European Union, APEC,  Organization for Economic Cooperation and Development (OECD), and other stakeholders, to evaluate how  existing and proposed policy frameworks address big data.It recommends strengthening the U.S.-European Union Safe Harbor Framework and encourages more countries and companies to join the APEC Cross Border  Privacy Rules system.
--------- Summary by Kullback–Leibler algorithm --------- 
As a result, big data analytics have the potential to eclipse longstanding civil rights protections in how personal  information is used in housing, credit, employment, health, education, and the marketplace.Federal agencies should implement best practices for  institutional protocols and mechanisms to ensure the controlled use and secure storage of data.Federal agencies with expertise in  privacy and data practices should provide technical assistance to state, local, and other federal law enforcement  agencies seeking to deploy big data techniques.The traditional concepts of notice and consent, which have been a key requirement in all data protection  regimes, may no longer be sufficient to protect personal privacy.More than ten years after California passed the first Security Breach Disclosure Law, the Federal legislators have  not been able to pass a law that would provide a uniform approach nationwide.After having been the target of much criticism for its practices and its lack of “adequate protection,” the United  States is now stepping up its efforts to communicate with other worldwide powers and attempt to establish, and  participate in, bridges between the different privacy and data protection regimes, such as through its initiatives  as part of the Asia Pacific Economic Cooperation (APEC).However, big data analytics and technologies - especially when combined with the new means of collecting  personal information such as sensors, wearable technologies, smart grid, or Internet of things devices - create  the potential for new uses of data.Some of these uses may be invasive, and erode privacy rights.
--------- Summary by LexRank algorithm --------- 
The Report identifies five  areas of focus:  protecting privacy, preventing discrimination, ensuring responsible use of information by  government agencies, harnessing data as a public resource, and using big data to enhance learning  opportunities.The Report concludes with policy recommendations, including, advancing the Consumer Privacy  Bill of Rights; passing national data breach legislation; amending the Electronic Communications Privacy Act;  expanding technical expertise to stop discrimination; extending privacy protections to non-U.S. persons; and  ensuring that data collected on students in school are used for educational purposes.Federal agencies with expertise in  privacy and data practices should provide technical assistance to state, local, and other federal law enforcement  agencies seeking to deploy big data techniques.The Report recommends the adoption of a national data privacy law that would incorporate the principles laid  out in the White House Consumer Privacy Bill of Rights.After having been the target of much criticism for its practices and its lack of “adequate protection,” the United  States is now stepping up its efforts to communicate with other worldwide powers and attempt to establish, and  participate in, bridges between the different privacy and data protection regimes, such as through its initiatives  as part of the Asia Pacific Economic Cooperation (APEC).It recommends strengthening the U.S.-European Union Safe Harbor Framework and encourages more countries and companies to join the APEC Cross Border  Privacy Rules system.Big data may create tools or information that may lead to discrimination.However, big data analytics and technologies - especially when combined with the new means of collecting  personal information such as sensors, wearable technologies, smart grid, or Internet of things devices - create  the potential for new uses of data.

 ************ Summaries for file before_Comment_on_Big_Data_Future_of_Privacy.txt. ************ 

--------- Summary by usual TextRank algorithm --------- 
For example, do the current U.S. policy framework and privacy proposals for protecting  consumer privacy and government use of data adequately address issues raised by big data  analytics?The policy framework needs to address systematizing privacy preserving data disclosure  mitigating linkability threats in the big data era.Jobs data matching geo location data and education data will lead to better employment outcomes.Big data analytics on encrypted data to thwart linkability threats.Big data storage in the cloud across multiple geo boundaries Lack of transparency: who has access to which data, which data is collected and for what reason.The Cloud Security Alliance (CSA) Big Data Working Group (BDWG) has come up with 100 best practices to  enhance the security and privacy of big data.The policy frameworks and regulations for handling big data differ between the government and private entities  in the context of the ability of the government to adjudicate -- for example, in policies governing demographics  data in law enforcement and government investment of tax dollars.The question of where the data is stored, where the data is processed and where the data analytics results are  distributed influence the cross boundary jurisdictions pertaining to privacy policies and regulations.
--------- Summary by LSA algorithm --------- 
Public policy implications have a bearing on Access, Ownership, Privacy, Liability, and Transparency.Privacy protection has become an elusive goal in the big data era as researchers have shown that "linkability  threats" can re-identity individuals.In practice, such  data is shared after sufficient removal of unique identifiers by the processes of anonymization and aggregation.Smarter city and transport infrastructure, leading to a most cost effective and greener environment,  lesser and faster commute.Tracking consumer behavior and sharing them with 3 rd party without proper authorization for targeting  and other purposes.Healthcare Education, Financial wellness, Employment, Mobility, and Information Access are some of the  specific sectors that should receive more government/public attention.Are there particularly promising technologies or new practices for safeguarding privacy while enabling effective  uses of big data?It should be noted that although technologies  play an important role in the safeguarding of privacy, the approach should also include, amongst others, legal  and administrative aspects.
--------- Summary by Kullback–Leibler algorithm --------- 
Existing practices focus on keeping data  encrypted at rest and in transit with an infrastructure to ensure proper authorization and authentication of  entities to get access to the data.In practice, such  data is shared after sufficient removal of unique identifiers by the processes of anonymization and aggregation.As well as enforcement of transparency: individuals have  the right to know which party has which access to their data, how the (raw) data is used and how it is protected.Jobs data matching geo location data and education data will lead to better employment outcomes.Sharing cyber threat intelligence among multiple businesses will lead to thwarting potential cyber  threats to national infrastructure.Big data analytics on encrypted data to thwart linkability threats.Correlation of disparate data such as healthcare, financial, demographic and location data.Tracking consumer behavior and sharing them with 3 rd party without proper authorization for targeting  and other purposes.
--------- Summary by LexRank algorithm --------- 
For example, do the current U.S. policy framework and privacy proposals for protecting  consumer privacy and government use of data adequately address issues raised by big data  analytics?Specifically, the policy framework might have to lead to  enforcement that all linkable data be encrypted.Furthermore, the policy framework needs to address the  concerns on the geo location where the data is stored.As well as enforcement of transparency: individuals have  the right to know which party has which access to their data, how the (raw) data is used and how it is protected.Big data analytics on encrypted data to thwart linkability threats.This should also  result in a more cost effective health care system.Big data storage in the cloud across multiple geo boundaries Lack of transparency: who has access to which data, which data is collected and for what reason.Homomorphic Encryption and Differential Privacy are some of the promising technologies for  safeguarding privacy while enabling effective uses of big data.

 ************ Summaries for file before_CSA13-Top10Crypto.txt. ************ 

--------- Summary by usual TextRank algorithm --------- 
How is cryptography for Big Data different?DATA: balance privacy and utility, enable analytics and governance on encrypted data, reconcile authentication and anonymity.Key Points: Secure Dissemination - Differential Privacy, Policy-based Encryption, Searching/Filtering Encrypted Data, Secure Outsourcing of Computation, Proof of Data Storage.Traditionally access control has been enforced by systems – Operating Systems, Virtual Machines: Restrict access to data, based on access policy, Data is still in plaintext, Systems can be hacked, Security of the same data in transit is a separate concern.Encrypted data can be moved around, as well as kept at rest – uniform handling.Searching and Filtering Encrypted Data.There is a realization across the industry that cryptographic technologies are imperative for cloud and big data.Sophisticated techniques are in research stage or have limited deployments, which enable rich transformations and management of encrypted data.
--------- Summary by LSA algorithm --------- 
Traditionally access control has been enforced by systems – Operating Systems, Virtual Machines: Restrict access to data, based on access policy, Data is still in plaintext, Systems can be hacked, Security of the same data in transit is a separate concern.What if we protect the data itself in a cryptographic shell depending on the access policy?Suppose you have a system to receive emails encrypted under your public key.Cloud can perform any computation on the underlying plaintext, all the while the results are encrypted!There is a realization across the industry that cryptographic technologies are imperative for cloud and big data.Mathematical assurance of trust gives people more incentive to migrate data and computation to cloud.Systematic and mathematical considerations need to applied when responding to queries on personal data.Sophisticated techniques are in research stage or have limited deployments, which enable rich transformations and management of encrypted data.
--------- Summary by Kullback–Leibler algorithm --------- 
BIG: scale up existing solutions for volume, variety and velocity, retarget to Big Data infrastructural shift.Key Points: Secure Dissemination - Differential Privacy, Policy-based Encryption, Searching/Filtering Encrypted Data, Secure Outsourcing of Computation, Proof of Data Storage.Roughly speaking: Ensures that almost no risk is incurred by joining a statistical database, Ensures that the removal or addition of an item does not significantly alter the outcome of any analysis.Traditionally access control has been enforced by systems – Operating Systems, Virtual Machines: Restrict access to data, based on access policy, Data is still in plaintext, Systems can be hacked, Security of the same data in transit is a separate concern.What if we protect the data itself in a cryptographic shell depending on the access policy?Decryption only possible by entities allowed by the policy.Keys can be hacked!Systematic and mathematical considerations need to applied when responding to queries on personal data.
--------- Summary by LexRank algorithm --------- 
BIG: scale up existing solutions for volume, variety and velocity, retarget to Big Data infrastructural shift.DATA: balance privacy and utility, enable analytics and governance on encrypted data, reconcile authentication and anonymity.Key Points: Secure Dissemination - Differential Privacy, Policy-based Encryption, Searching/Filtering Encrypted Data, Secure Outsourcing of Computation, Proof of Data Storage.What if we protect the data itself in a cryptographic shell depending on the access policy?Suppose you have a system to receive emails encrypted under your public key.But wouldn’t be much use if you wanted the cloud to perform some computations on them.Mathematical assurance of trust gives people more incentive to migrate data and computation to cloud.Sophisticated techniques are in research stage or have limited deployments, which enable rich transformations and management of encrypted data.

 ************ Summaries for file iso_N0200_ISO-IEC_20546_Committee_Draft.txt. ************ 

--------- Summary by usual TextRank algorithm --------- 
This international standard will provide the conceptual overview of the emerging field of Big Data, its relationship to other technical areas and standards efforts, and the concepts ascribed to big data that are not new to Big Data.Volatility in Big Data usage refers to data volatility, or the rate of change of data over time.This term is not new to big data, but is often ascribed to big data due to the understanding that data has potential value that was typically not considered previously.However, the big data paradigm shift has increased the emphasis on the value of unstructured or relationship data, and also on different engineering methods that can handle data more efficiently.Metadata is data about data or data elements, including the description of the processing history of the data.As Big Data systems are architected to perform distributed data processing including data that is external and not under the control of the big data system, the use of metadata becomes an increasingly important concept.The development of algorithms for the analysis of data previously did not consider the requirements of distributed data processing, the data was typically held locally.This makes it hard for individuals to use their data for their own sake, and for organizations to use personal data owned by other organizations, so that noone can utilize big and deep personal data.
--------- Summary by LSA algorithm --------- 
In the field of information technology, ISO and IEC have established a joint technical committee, ISO/IEC JTC 1. International Standards are drafted in accordance with the rules given in the ISO/IEC Directives, Part 2.The revolution in technologies referred to as big data has arisen because the relational model could no longer efficiently handle all the needs for analysis of large and often unstructured datasets.Variability refers to changes in data rate, format/structure, semantics, and/or quality that impact the supported application, analytic, or problem.Cloud computing is a paradigm for enabling network access to a scalable and elastic pool of shareable physical or virtual resources with self-service provisioning and administration on demand.There are several key characteristics often present in for cloud computing deployments including: broad network access, measured service, multi-tenancy, on-demand self-service, rapid elasticity and scalability, and resource pooling.SQL was first published as ISO International Standard (ISO/IEC 9075) in 1987, and it has been revised to include a larger set of features as the query language for Information technology.SQL provides a mature and comprehensive framework for data access supporting a broad range of advanced analytical features.This is in contrast to high volume big data systems that typically run batch jobs over a relatively small number of large datasets.
--------- Summary by Kullback–Leibler algorithm --------- 
This international standard will provide the conceptual overview of the emerging field of Big Data, its relationship to other technical areas and standards efforts, and the concepts ascribed to big data that are not new to Big Data.This International Standard provides an overview of Big Data along with a set of terms and definitions.The following International Standards contain provisions which, through reference in this text, constitute provisions of this International Standard.This term is not new to big data, but is often ascribed to big data due to the understanding that data has potential value that was typically not considered previously.Metadata is data about data or data elements, including the description of the processing history of the data.As Big Data systems are architected to perform distributed data processing including data that is external and not under the control of the big data system, the use of metadata becomes an increasingly important concept.Typical technological advances in sensors, and the deployment of IPV6 to provide Internet connectivity to sensors creates the need for a big data system that can handle high velocity streaming data from a number of sources.The needs for distributed data processing have led to a number of new programming and query languages suited to the development of big data systems, as well as new processes.
--------- Summary by LexRank algorithm --------- 
This international standard will provide the conceptual overview of the emerging field of Big Data, its relationship to other technical areas and standards efforts, and the concepts ascribed to big data that are not new to Big Data.This International Standard provides an overview of Big Data along with a set of terms and definitions.The revolution in technologies referred to as big data has arisen because the relational model could no longer efficiently handle all the needs for analysis of large and often unstructured datasets.As Big Data systems are architected to perform distributed data processing including data that is external and not under the control of the big data system, the use of metadata becomes an increasingly important concept.Cloud computing is an infrastructure model for the development of a Big Data system.SQL was first published as ISO International Standard (ISO/IEC 9075) in 1987, and it has been revised to include a larger set of features as the query language for Information technology.Big Data typically refers to distributed data-intensive processing across the nodes of a cluster.The needs for distributed data processing have led to a number of new programming and query languages suited to the development of big data systems, as well as new processes.

 ************ Summaries for file iso_ISO-IECJTC1-WG9_N0087_N0087_WD_of_ISOIEC_20546_1st_Edition.txt. ************ 

--------- Summary by usual TextRank algorithm --------- 
The ISO/IEC JTC1 WG9 on Big data decided that the normative content of this document would benefit greatly at this time with another informative document that provided context for the creation of 20546, as well as discussion of the concepts ascribed to Big Data that are not in fact new to Big Data.The technical report will provide the conceptual overview of the emerging field of Big Data, its relationship to other technical areas and standards efforts, and the concepts ascribed to big data that are not new to Big Data.The current revolution in technologies referred to as Big Data has arisen because the relational data model can no longer efficiently handle all the current needs for analysis of large and often unstructured datasets.The term Big Data is often used to refer to a new field of study, that encompasses Big Data engineering, data science, and data platforms.Additional concepts are equated to Big Data, which in fact are not new and are not unique to this new Big Data paradigm.This term is not new to big data, but is often ascribed to Big Data due to the understanding that data has intrinsic value that was typically not considered previously.However, the Big Data paradigm shift has increased the emphasis on the value of unstructured or relationship data, and also on different engineering methods that can handle data more efficiently.A.1.1 ISO/IEC 17788:2014, Information technology — Cloud computing – Overview and vocabulary A.1.2 ISO\IEC JTC1 Information technology Big data – preliminary report 2014 A.2 Guiding ISO\IEC Documents A.2.1 ISO 704:200, Terminology work – Principles and methods A.2.2 ISO 860:2007 Terminology work – Harmonization of concepts and terms A.2.3 ISO 1087-1:2000, Terminology work -- Vocabulary -- Part 1: Theory and application A.2.4 ISO 10241-1:2011 Terminology entries in standards – Part 1: General Requirements and examples of presentation A.2.5 ISO 10241-2:2011 Terminology entries in standards – Part 2: Adoption of standardised terminological entries A.2.6 JTC 1 SD 20 JTC 1 IT Vocabulary Maintenance Team Best Practices Guide, Executive Summary (JTC 1 N 12090).
--------- Summary by LSA algorithm --------- 
Information Technology – Big Data – Overview and Vocabulary (Updates after July WG9 F2F Meeting in Spain).This standard will provide the normative definitions and the vocabulary needed to promote improved communication and understanding of this emerging area.This International Standard provides an overview of Big Data along with a set of terms and definitions.For undated references, the latest edition of the referenced document (including any amendments) applies.The term does not, however, represent data that is simply bigger than before, since this has happened on a regular basis for decades.The specific occurrence that has led to the widespread usage of this term is that these extensive datasets can no longer be handled in traditional data system architectures.The term big data implies the paradigm shift to use distributed, scalable computing for data-intensive systems to achieve the needed performance efficiency at an affordable cost.To understand this revolution, the interplay of the following aspects must be considered: the characteristics of the datasets, the analysis of the datasets, the performance of the systems that handle the data, the business considerations of cost effectiveness, and the new technical field of engineering and analysis techniques for distributed processing across nodes.
--------- Summary by Kullback–Leibler algorithm --------- 
The ISO/IEC JTC1 WG9 on Big data decided that the normative content of this document would benefit greatly at this time with another informative document that provided context for the creation of 20546, as well as discussion of the concepts ascribed to Big Data that are not in fact new to Big Data.The technical report will provide the conceptual overview of the emerging field of Big Data, its relationship to other technical areas and standards efforts, and the concepts ascribed to big data that are not new to Big Data.The term Big Data implies data that is extensive in volume, velocity, or variety.Big Data velocity means a large quantity of data is being processed in a short amount of time.The term Big Data is often used to refer to a new field of study, that encompasses Big Data engineering, data science, and data platforms.Additional concepts are equated to Big Data, which in fact are not new and are not unique to this new Big Data paradigm.This term is not new to big data, but is often ascribed to Big Data due to the understanding that data has intrinsic value that was typically not considered previously.However, the Big Data paradigm shift has increased the emphasis on the value of unstructured or relationship data, and also on different engineering methods that can handle data more efficiently.
--------- Summary by LexRank algorithm --------- 
For an explanation on the meaning of ISO specific terms and expressions related to conformity assessment, as well as information about ISO's adherence to the WTO principles in the Technical Barriers to Trade (TBT) see the following URL: Foreword - Supplementary information The committee responsible for this document is ISO/JTC1 Information Technology, Working Group WG9, Big Data.The technical report will provide the conceptual overview of the emerging field of Big Data, its relationship to other technical areas and standards efforts, and the concepts ascribed to big data that are not new to Big Data.The following documents, in whole or in part, are normatively referenced in this document and are indispensable for its application.The term Big Data implies data that is extensive in volume, velocity, or variety.In the evolution of data systems, there have been a number of times when the need for efficient, cost effective data analysis has forced a change in existing technologies.Should this trend continue, by 2020 there would be 500 times the amount of data as existed in 2011.Variety represents the need to analyse across a number of data domains and a number of data types.However, the Big Data paradigm shift has increased the emphasis on the value of unstructured or relationship data, and also on different engineering methods that can handle data more efficiently.

 ************ Summaries for file iso_N0147_ISO_IEC_20546_2nd_WorkingDraft.txt. ************ 

--------- Summary by usual TextRank algorithm --------- 
For an explanation on the meaning of ISO specific terms and expressions related to conformity assessment, as well as information about ISO's adherence to the WTO principles in the Technical Barriers to Trade (TBT) see the following URL: Foreword - Supplementary information The committee responsible for this document is ISO/JTC1 Information Technology, Working Group WG9, Big Data.The ISO/IEC JTC1 WG9 on Big data decided that the normative content of this document would benefit greatly at this time with another informative document that provided context for the creation of 20546, as well as discussion of the concepts ascribed to Big Data that are not in fact new to Big Data.The technical report will provide the conceptual overview of the emerging field of Big Data, its relationship to other technical areas and standards efforts, and the concepts ascribed to big data that are not new to Big Data.The term Big Data implies data that is extensive in volume, velocity, or variety.The current revolution in technologies referred to as Big Data has arisen because the relational data model can no longer efficiently handle all the current needs for analysis of large and often unstructured datasets.Many of the conceptual underpinnings of Big Data have been around for years, but the last decade has seen an explosion in their maturation and application to scaled data systems.Customers may develop their own Big Data system on top of PaaS or IaaS, or use Big Data service via SaaS.Analytics is a must-have functional component of data warehouse and big data.
--------- Summary by LSA algorithm --------- 
The specific occurrence that has led to the widespread usage of this term is that these extensive datasets can no longer be handled in traditional data system architectures.The term big data implies the paradigm shift to use distributed, scalable computing for data-intensive systems to achieve the needed performance efficiency at an affordable cost.Cloud computing is a paradigm for enabling network access to a scalable and elastic pool of shareable physical or virtual resources with self-service provisioning and administration on demand.There are several key characteristics often present in for cloud computing deployments including: broad network access, measured service, multi-tenancy, on-demand self-service, rapid elasticity and scalability, and resource pooling.Most Big Data functional components are provided by Cloud computing, helping systems achieve characteristics of volume, velocity, and variety.For small sample sizes, it has been shown in a number of cases that individuals can be identified from their activity, for example in health datasets or in movies watched.SQL was first published as ISO International Standard (ISO/IEC 9075) in 1987, and it has been revised to include a larger set of features as the query language for Information technology.SQL provides a mature and comprehensive framework for data access supporting a broad range of advanced analytical features.
--------- Summary by Kullback–Leibler algorithm --------- 
The ISO/IEC JTC1 WG9 on Big data decided that the normative content of this document would benefit greatly at this time with another informative document that provided context for the creation of 20546, as well as discussion of the concepts ascribed to Big Data that are not in fact new to Big Data.The technical report will provide the conceptual overview of the emerging field of Big Data, its relationship to other technical areas and standards efforts, and the concepts ascribed to big data that are not new to Big Data.The term Big Data implies data that is extensive in volume, velocity, or variety.Many of the conceptual underpinnings of Big Data have been around for years, but the last decade has seen an explosion in their maturation and application to scaled data systems.Customers may develop their own Big Data system on top of PaaS or IaaS, or use Big Data service via SaaS.This problem pre-dated the big data regime.SQL is designed for manipulating structured data, and it is also fast becoming the default language for big data analytics.Analytics is a must-have functional component of data warehouse and big data.
--------- Summary by LexRank algorithm --------- 
For an explanation on the meaning of ISO specific terms and expressions related to conformity assessment, as well as information about ISO's adherence to the WTO principles in the Technical Barriers to Trade (TBT) see the following URL: Foreword - Supplementary information The committee responsible for this document is ISO/JTC1 Information Technology, Working Group WG9, Big Data.The technical report will provide the conceptual overview of the emerging field of Big Data, its relationship to other technical areas and standards efforts, and the concepts ascribed to big data that are not new to Big Data.This International Standard provides an overview of Big Data along with a set of terms and definitions.The following documents, in whole or in part, are normatively referenced in this document and are indispensable for its application.Cloud computing is an accepted infrastructure model for development of a Big Data system.SQL was first published as ISO International Standard (ISO/IEC 9075) in 1987, and it has been revised to include a larger set of features as the query language for Information technology.A.1 Related ISO\IEC Standards.A.2 Guiding ISO\IEC Documents.

 ************ Summaries for file itu_T-REC-Y3600-201511.txt. ************ 

--------- Summary by usual TextRank algorithm --------- 
Keywords: Big data, big data ecosystem, cloud computing, data analytics, data storage, real-time analysis.Big data service provider activities include:  searching  data  sources  (from  the  data  broker)  and  collecting  data  by  requesting  and  crawling, storing data to a data repository, integrating data, providing tools for data analysis and visualization, supporting  data  management  such  as  data  provenance,  data  privacy,  data  security,  data  retention policy, data ownership, etc.Big data technologies and services are  challenged  to  protect  personal  identities  and  sensitive  attributes  of  data  throughout  the  whole data processing cycle while respecting applicable data retention policy.This  clause  describes  how  cloud  computing can support  the three main roles in  a  big data ecosystem:  data provider, big data service  provider and big data service customer.The data collection requirements include: It  is  required  for  the  CSP:BDIP  to  support  collecting  data  from  multiple  CSN:DPs  in  parallel, It  is  recommended  for  the  CSN:DP  to  expose  data  to  the  CSP:BDAP  by  publishing  metadata, It  is  recommended  that  the  CSP:BDIP  supports  collecting  data  from  different  CSN:DPs  with different modes, NOTE  –  Data could be collected in different modes, such as pull mode in which the  data collection  process is initiated by CSP:BDIP,  or push mode in which the  data collection process is initiated by  It  is  recommended  for  the  CSN:DP  to  provide  a  brokerage  service  to  the  CSP:BDIP  for  searching accessible data, NOTE  –  Brokerage provides data  a  catalog which has  data  information such as data specification,  data instructions, electronic access methods, license policy, data quality, etc.The data analysis requirements include: It is required for the CSP:BDAP to support analysis of various data types and formats, It is required for the CSP:BDAP to support batch processing, It is required for the CSP:BDAP to support association analysis, It is required for the CSP:BDAP to support different data analysis algorithms, NOTE – Data analysis algorithms include classification, clustering, regression, association, ranking, etc.Data collection capabilities include: Data source intelligent recognition, which  offers the capabilities to locate the data sources  and detect the types of data being collected, Data  adaptation,  which  offers  the  capabilities  to  transform  and  organize  the  data  being  collected  with  targeted  data  structures  and  attributes  (numbering,  location,  ownerships,  etc.), Data integration, which offers the capabilities to integrate data from different data sources  (different data types) using metadata or ontology, Data brokerage, which offers the capabilities to provide a brokerage service for searching  data.
--------- Summary by LSA algorithm --------- 
Recommendation  ITU-T  Y.3600  provides  requirements,  capabilities  and  use  cases  of  cloud  computing based big data as well as its system context.ITU-T is responsible for studying technical,  operating  and  tariff  questions  and  issuing  Recommendations  on  them  with  a  view  to  standardizing  telecommunications on a worldwide basis.In  some  areas  of  information  technology  which  fall  within  ITU -T's  purview,  the  necessary  standards  are  prepared on a collaborative basis with ISO and IEC.ITU  draws  attention  to  the  possibility  that  the  practice  or  implementation  of  this  Recommendation  may  involve  the  use  of  a  claimed  Intellectual  Property  Right.In the body of this document and its annexes, the words shall, shall not, should, and may sometimes  appear, in which case they are to be interpreted, respectively, as is required to, is prohibited from, is  recommended, and can optionally.Taking  into  account  the  above  Vs'  described  characteristics,  big  data  technologies  and  services  allow many new challenges to be resolved and also create more new opportunities than ever before.Positive  resolving  of  the  above  challenges  opens  new  opportunities  to  discover  new  data  relationships, hidden patterns or unknown dependencies.Big data refers  to  technologies and services which extract valuable information from the  extensive datasets  characterized  by  the  Vs,  while  cloud  computing  is,  as  defined  in  [ITU-T Y.3500],  the  paradigm for enabling network access to a scalable and elastic pool of shareable physical or virtual  resources with self-service provisioning and administration on-demand.
--------- Summary by Kullback–Leibler algorithm --------- 
Big data – Cloud computing based requirements  and capabilities.Keywords: Big data, big data ecosystem, cloud computing, data analytics, data storage, real-time analysis.Cloud computing based big data capabilities.Big data service provider activities include:  searching  data  sources  (from  the  data  broker)  and  collecting  data  by  requesting  and  crawling, storing data to a data repository, integrating data, providing tools for data analysis and visualization, supporting  data  management  such  as  data  provenance,  data  privacy,  data  security,  data  retention policy, data ownership, etc.The  big  data  ecosystems,  which  are  supported  by  a  cloud  computing  system context,  can be  referred to  as cloud computing based big data.This  clause  describes  a  cloud  computing  based  big  data  system  context  that  is  effective  for  supporting big data.This  clause  describes  how  cloud  computing can support  the three main roles in  a  big data ecosystem:  data provider, big data service  provider and big data service customer.), Data integration, which offers the capabilities to integrate data from different data sources  (different data types) using metadata or ontology, Data brokerage, which offers the capabilities to provide a brokerage service for searching  data.
--------- Summary by LexRank algorithm --------- 
Recommendation ITU-T Y.3600.Cloud computing based big data provides the  capabilities to collect, store,  analyse,  visualize and  manage varieties of large volume datasets, which  cannot be rapidly transferred and analysed using traditional technologies.Keywords: Big data, big data ecosystem, cloud computing, data analytics, data storage, real-time analysis.The  ITU  Telecommunication  Standardization Sector (ITU-T) is a permanent organ of ITU.Volume: refers to the amount of data collected, stored, analysed and visualized, which big  data technologies need to resolve.Big data service provider activities include:  searching  data  sources  (from  the  data  broker)  and  collecting  data  by  requesting  and  crawling, storing data to a data repository, integrating data, providing tools for data analysis and visualization, supporting  data  management  such  as  data  provenance,  data  privacy,  data  security,  data  retention policy, data ownership, etc.The  big  data  ecosystems,  which  are  supported  by  a  cloud  computing  system context,  can be  referred to  as cloud computing based big data.This  clause  describes  how  cloud  computing can support  the three main roles in  a  big data ecosystem:  data provider, big data service  provider and big data service customer.

 ************ Summaries for file itu_ITUbroshure.txt. ************ 

--------- Summary by usual TextRank algorithm --------- 
Billions of  individual pieces of data are amassed each day, from sources including supplier data, delivery slips,  restaurant locations, employment records, DNA  records, data from Interpol’s database of  international criminals, and also customer complaints and user-generated content such as location  check-ins, messages, photos and videos on social media sites.The Global Health Data Exchange is such a go-to repository for population  health data enriched by a set of tools to visualize and explore the data.The Open  Knowledge Foundation defines openness in the context of open data: “A piece of data or content is  open if anyone is free to use, reuse, and redistribute it –subject only, at most, to the requirement to  attribute and/or share-alike.” The political leaders of the G8 – Canada, France, Germany, Italy, Japan, Russia, the United Kingdom  and the United States of America – have set out five strategic principles aimed at unlocking the  economic potential of open data, supporting innovation, and providing greater accountability.Distributed file systems, programming models and scalable high-performance databases are big data  core technologies exploited in stages 2 and 3 of the simplified big data value chain.The back-end allows  researchers to query data via free text-based queries, then it aggregates the results from across  distributed providers and represents the data in a defined format, all the while preserving integrity,  security policy and data formats.Big data stands in stark contrast to data avoidance and data minimization, two basic principles of  data protection.The NIST working group intends to support secure and effective adoption of  big data by developing consensus on definitions, taxonomies, secure reference architectures and a  technology roadmap for big data analytic techniques and technology infrastructures.With  the  boom of personal and wearable ‘connected health’ and fitness products in mind, if a smart wristband  could exchange data with a smartwatch of a different make (uninhibited by vendor or manufacturer  boundaries), big data would benefit with the ability to pull together and securely integrate data  collected on different devices.
--------- Summary by LSA algorithm --------- 
Big data – a composite term describing emerging technological capabilities in solving complex tasks –  has been hailed by industry analysts, business strategists and marketing pros as a new frontier for  innovation, competition and productivity.Health data is vital  in getting this message across to policy makers, aid organizations and donors, but, no matter how  accurate and complete raw statistics and endless spreadsheets may be, their form is not one that  lends itself to easy analysis and interpretation.The initiative is exploring how new, digital data sources and real-time  analytics technologies can help policymakers understand human well-being and emerging  vulnerabilities in real time, in the interests of better protecting populations from the aftershock of  financial and political crises.Each of these consumers contribute to the data deluge, with SMS; calls; photos,  videos and messages posted on social media sites; emails; searches; clicks on links and ads; online  shopping and mobile payments; or location traces left by GPS-enabled smartphones and WiFi  network logins.Data philanthropy requires precise and  suitable legal frameworks, ethical guidelines and trusted technology solutions to safeguard the  highest standards for the preservation of privacy – a critical challenge we will address in the  following chapter.Large sets of mobile call records, even when anonymized and stripped of all personal information,  can be used to create highly unique fingerprints ofusers, which in combination with other data such  as geo-located tweets or “check-ins” may help to reveal the individual.There is rapid progress in all  these areas, and we see that the latest generations of products and solutions feature unprecedented  performance and scalability in handling, processing and making sense of data while meeting the  growing demands of Volume, Velocity, Variety and Veracity.This concerns  communications protocols, gateways and data integration platforms alike, and care should be taken  to avoid vendor lock-ins that deny consumers  the use of the latest data-driven third party  applications to reduce energy consumption or heat the house more efficiently.
--------- Summary by Kullback–Leibler algorithm --------- 
Analyzing global disease patterns and identifying trendsat an early stage is mission critical for actors  in the pharmaceutical and medical products sector, allowing them to model future demand and costs  for their products and so make strategic R&D investment decisions.Combined with other medical  datasets, they allow scientists to analyze biological pathways systematically, leading to an  understanding of how these pathways could be manipulated to treat disease.City and  transport planning was one of the themesof the ‘Data for Development’ challenge launched  by  telecommunication provider Orange in summer 2012.Participants were given access to anonymized  datasets provided by the company’s Côte d’Ivoire branch which contained 2.5 billion records of calls  and text messages exchanged between 5 million users over a period of 5 months.Launched by the Executive Office of the United Nations SecretaryGeneral in the wake of “The Great Recession”, Global Pulse is an innovation initiative established in  response to the need for more timely information to track and monitor the impacts of global and  local socio-economic crises.By extracting detailed traffic information in real time, network analytics help providers to optimize  their routing network assets and to predict faults and bottlenecks before they cause any harm.It may represent a direct user experience,  such as a warning, notification or visualization , or could result in communication with a smart  system, for example by triggering a financial transaction or by adjusting traffic signals to reflect the  actual traffic flow.At present, ITU’s standardization activities address individual infrastructure requirements, noting  existing work in domains including optical transport and access networks, future network capabilities  (e.g., software-defined networks), multimedia and security.Previous reports in the series include: ICTs and Climate Change Ubiquitous Sensor Networks Remote Collaboration Tools NGNs and Energy Efficiency Distributed Computing: Utilities, Grids & Clouds The Future Internet Biometrics and Standards Decreasing Driver Distraction The Optical World Trends in Video Games and Gaming Digital Signage  Privacy in Cloud Computing E-health Standards and Interoperability E-learning Smart Cities Mobile Money  Spatial Standards.
--------- Summary by LexRank algorithm --------- 
Automated experiments generate very large amounts of data about disease mechanisms and they  deliver data of great importance in the early stages of drug discovery.As it drafts a new urban transport plan for individual  and collective means of transportation, call records offer an informative set of data on the mobility  of the population.CERN uses MongoDB as the primary back-end for the data aggregation of one of the LHC particle  detectors.The time is ripe to review information security policies, privacy  guidelines, and data protection acts.In 2012,  the Cloud Security Alliance established a big data working group with the aim of identifying scalable  techniques for data-centric security and privacy problems.At present, ITU’s standardization activities address individual infrastructure requirements, noting  existing work in domains including optical transport and access networks, future network capabilities  (e.g., software-defined networks), multimedia and security.A work item has been initiated tostudy the relationship between cloud computing and big data in  view of requirements and capabilities.A previous report in the Technology Watch series advocated the use  of privacy-enhancing technologies as means to implement the ‘privacy by design’ principle, which is  of course of great interest to big data applications.

 ************ Summaries for file nist_NISTSP1500-4.txt. ************ 

--------- Summary by usual TextRank algorithm --------- 
To advance progress in Big Data, the NIST Big Data Public Working Group (NBD-PWG) is  working to develop consensus on important, fundamentalconcepts related to Big Data.The NIST Big Data Public Working Group (NBD-PWG) Definitions and Taxonomy Subgroup prepared  this NIST Big Data Interoperability Framework: Volume 1, Definitionsto address fundamental concepts  needed to understand the new paradigm for data applications, collectively known as Big Data, and the  analytic processes collectively known as data science.While Big Data has been defined in a myriad of  ways, the shift to a Big Data paradigm occurs when the scale of the data leads to the need for a cluster of  computing and storage resources to provide cost-effective data management.The major steps involved in this effort included:  Announce that the NBD-PWG Security and Privacy Subgroup is open to the public in order to  attract and solicit a wide array of subject matter experts and stakeholders in government, industry,  and academia,  Identify use cases specific to Big Data security and privacy,  Develop a detailed security and privacy taxonomy,  Expand the security and privacy fabric of the NBDRA and identify specific topics related to  NBDRA components, and  Begin mapping of identified security and privacy use cases to the NBDRA.These topics include the following:  Examining closely other existing templates in literature: The templates may be adapted to the  Big Data security and privacy fabric to address gaps and to bridge the efforts of this Subgroup  with the work of others,  Further developing the security and privacy taxonomy,  Enhancing the connection between the security and privacy taxonomy and the NBDRA  components,  Developing the connection between the security and privacy fabric and the NBDRA,  Expanding the privacy discussion within the scope of this volume,  Exploring governance, risk management, data ownership, and valuation with respect to Big Data  ecosystem, with a focus on security and privacy,  Mapping the identified security and privacy use cases to the NBDRA,  Contextualizing the content of Appendix B in the NBDRA, and  Exploring privacy in actionable terms based on frameworks such as those described in NISTIR  with respect to the NBDRA.One of the primary objectives of this document is to  understand how Big Data security and privacy requirements arise out of the defining characteristics of  Big Data, and how these requirements are differentiated from traditional security and privacy  requirements.A candidate set of topics from the Cloud SecurityAlliance Big Data Working Group (CSA BDWG)  article, Top Ten Challenges in Big Data Security and Privacy Challenges, was used in developing these  security and privacy taxonomies.NBD-PWG decided to include the Data Provider and Data  Consumer as well as the Big Data Application and Framework Providers in the Security and Privacy  Fabric because these entities should agree on the security protocols and mechanisms in place.
--------- Summary by LSA algorithm --------- 
Despite widespread agreement on the inherent opportunities and current limitations of Big Data, a lack of  consensus on some important fundamental questions continues to confuse potential users and stymie  progress.The initiative’s goals include helping to accelerate the pace of discovery in  science and engineering, strengthening national security, and transforming teaching and learning by  improving the ability to extract knowledge and insights from large and complex collections of digital  data.Six federal departments and their agencies announcedmore than $200 million in commitments spread  across more than 80 projects, which aim to significantly improve the tools and techniques needed to  access, organize, and draw conclusions from huge volumes of digital data.Motivated by the White House initiative and public suggestions, the National Institute of Standards and  Technology (NIST) has accepted the challenge to stimulate collaboration among industry professionals to  further the secure and effective adoption of Big Data.An emergent phenomenon introduced by Big Data variety that has gained considerable importance is the  ability to infer identity from anonymized datasets by correlating with apparently innocuous public  databases.Any strategy to achieve proper access  control and security risk management within a Big Data cloud ecosystem enterprise architecture must  address the complexities associated with cloud-specific security requirements triggered by cloud  characteristics, including, but not limited to, the following:  Broad network access,  Decreased visibility and control by consumer,  Dynamic system boundaries and commingled roles and responsibilities between consumers and  providers,  Multi-tenancy, Data residency,  Measured service  Order-of-magnitude increases in scale (on demand), dynamics (elasticity and cost optimization),  and complexity (automation and virtualization).Current Security and Privacy Issues/Practices:Individual data is collected by several means, including  smartphone GPS (global positioning system) or location,browser use, social media, and applications  (apps) on smart devices.The Organization for the Advancement of  Structured Information Standards (OASIS) PrivacyManagement Reference Model (PMRM), consisting  of seven foundational principles, provides appropriate basic guidance for Big System architects.
--------- Summary by Kullback–Leibler algorithm --------- 
NIST Big Data Interoperability  Framework:  Volume 4, Security and Privacy.The NIST Big Data Public Working Group (NBD-PWG) Definitions and Taxonomy Subgroup prepared  this NIST Big Data Interoperability Framework: Volume 1, Definitionsto address fundamental concepts  needed to understand the new paradigm for data applications, collectively known as Big Data, and the  analytic processes collectively known as data science.The growth  rates for data volumes, speeds, and complexity are outpacing scientific and technological advances in data  analytics, management, transport, and data user spheres.These topics include the following:  Examining closely other existing templates in literature: The templates may be adapted to the  Big Data security and privacy fabric to address gaps and to bridge the efforts of this Subgroup  with the work of others,  Further developing the security and privacy taxonomy,  Enhancing the connection between the security and privacy taxonomy and the NBDRA  components,  Developing the connection between the security and privacy fabric and the NBDRA,  Expanding the privacy discussion within the scope of this volume,  Exploring governance, risk management, data ownership, and valuation with respect to Big Data  ecosystem, with a focus on security and privacy,  Mapping the identified security and privacy use cases to the NBDRA,  Contextualizing the content of Appendix B in the NBDRA, and  Exploring privacy in actionable terms based on frameworks such as those described in NISTIR  with respect to the NBDRA.Data aggregation and dissemination must be secured inside the context of a formal,  understandable framework.The availability of data and transparency of its current and past use  by data consumers is an important aspect ofBig Data.Because there may be disparate, potentially unanticipated processing steps  between the data owner, provider,and data consumer, the privacy and integrity of data coming  from end points should be protected at every stage.For  example, a data scientist has deep specialization in the content and its transformation, but may not focus  on security or privacy until it adds effort, cost, risk, or compliance responsibilities to the process of  accessing domain-specific data or analytical tools.The Data Provider and Data Consumer are included in the Security and  Privacy Fabric since, at the least, they should agree on the security protocols and mechanisms in place.
--------- Summary by LexRank algorithm --------- 
The NIST Big Data Public Working Group (NBD-PWG) Definitions and Taxonomy Subgroup prepared  this NIST Big Data Interoperability Framework: Volume 1, Definitionsto address fundamental concepts  needed to understand the new paradigm for data applications, collectively known as Big Data, and the  analytic processes collectively known as data science.The major steps involved in this effort included:  Announce that the NBD-PWG Security and Privacy Subgroup is open to the public in order to  attract and solicit a wide array of subject matter experts and stakeholders in government, industry,  and academia,  Identify use cases specific to Big Data security and privacy,  Develop a detailed security and privacy taxonomy,  Expand the security and privacy fabric of the NBDRA and identify specific topics related to  NBDRA components, and  Begin mapping of identified security and privacy use cases to the NBDRA.These topics include the following:  Examining closely other existing templates in literature: The templates may be adapted to the  Big Data security and privacy fabric to address gaps and to bridge the efforts of this Subgroup  with the work of others,  Further developing the security and privacy taxonomy,  Enhancing the connection between the security and privacy taxonomy and the NBDRA  components,  Developing the connection between the security and privacy fabric and the NBDRA,  Expanding the privacy discussion within the scope of this volume,  Exploring governance, risk management, data ownership, and valuation with respect to Big Data  ecosystem, with a focus on security and privacy,  Mapping the identified security and privacy use cases to the NBDRA,  Contextualizing the content of Appendix B in the NBDRA, and  Exploring privacy in actionable terms based on frameworks such as those described in NISTIR  with respect to the NBDRA.Effective communication across these  diverse industries will require standardization of the terms related to security and privacy.One of the primary objectives of this document is to  understand how Big Data security and privacy requirements arise out of the defining characteristics of  Big Data, and how these requirements are differentiated from traditional security and privacy  requirements.Managing data integrity for Big Data presents additional challenges related to all  the Big Data characteristics, but especially for PII.For example, confidentiality of communications can apply to governance of data at rest  and access management, but it is also part of a security metadata model.When working with any personal data, privacy should be an integral element in the design of a Big Data  system.

 ************ Summaries for file nist_NISTSP1500-1.txt. ************ 

--------- Summary by usual TextRank algorithm --------- 
The NIST Big Data Public Working Group (NBD-PWG) Definitions and Taxonomy Subgroup prepared  this NIST Big Data Interoperability Framework: Volume 1, Definitionsto address fundamental concepts  needed to understand the new paradigm for data applications, collectively known as Big Data, and the  analytic processes collectively known as data science.These questions include the following:  What attributes define Big Data solutions,  How is Big Data different from traditional dataenvironments and related applications,  What are the essential characteristics of Big Data environments,  How do these environments integrate withcurrently deployed architectures,  What are the central scientific, technological, and standardization challenges that need to be  addressed to accelerate the deployment of robust Big Data solutions,  Within this context, on March 29, 2012, the White House announced the Big Data Research and  Development Initiative.These Big Data characteristics dictate the overall design of a Big  Data system, resulting in different data system architectures or different data life cycle process orderings  to achieve needed efficiencies.These system concepts are not specific to Big Data, but their presence in  Big Data can be understood in the examination of a Big Data reference architecture, which is discussed in  NIST Big Data Interoperability Framework: Volume 6, Reference Architecture of this series.Data science across the entire data life cycle incorporates principles, techniques, and methods from many  disciplines and domains including data cleansing, data management, analytics, visualization, engineering,  and in the context of Big Data, now also includes Big Data Engineering.In the new Big Data paradigm, it is implied that data sampling from the overall data  population is no longer necessary since the Big Data system can theoretically process all the data without  loss of performance.As open data(data available to others) and linked data(data that is connected to other data) become the  norm, it is increasingly important to have information about how data was collected, transmitted, and  processed.The second characteristic of data at rest is the increasing need to use a variety of data, meaning the data  represents a number of data domains and a number of data types.
--------- Summary by LSA algorithm --------- 
Despite widespread agreement on the inherent opportunities and current limitations of Big Data, a lack of  consensus on some important fundamental questions continues to confuse potential users and stymie  progress.The initiative’s goals include helping to accelerate the pace of discovery in  science and engineering, strengthening national security, and transforming teaching and learning by  improving the ability to extract knowledge and insightsfrom large and complex collections of digital  data.Six federal departments and their agencies announcedmore than $200 million in commitments spread  across more than 80 projects, which aim to significantly improve the tools and techniques needed to  access, organize, and draw conclusions from huge volumes of digital data.Motivated by the White House initiative and public suggestions, the National Institute of Standards and  Technology (NIST) has accepted the challenge to stimulate collaboration among industry professionals to  further the secure and effective adoption of Big Data.This first version of  NIST Big Data Interoperability Framework: Volume 1, Definitionsdescribes some of the fundamental  concepts that will be important to determine categories or functional capabilities that represent  architecture choices.Improvements and “generalizations” of Map/Reduce have been  developed that provide additional functions lacking in the older technology, including fault tolerance,  iteration flexibility, elimination of middle layer, and ease of query.Of  special importance to resource management development are new features for supporting additional  processing models (other than Map/Reduce) and controls for multi-tenant environments, higher  availability, and lower latency applications.In external use cases, each end of the P2P systemcontributes bandwidth to the data movement, making  this currently the fastest way to leverage documents to the largest number of concurrent users.
--------- Summary by Kullback–Leibler algorithm --------- 
The NIST Big Data Public Working Group (NBD-PWG) Definitions and Taxonomy Subgroup prepared  this NIST Big Data Interoperability Framework: Volume 1, Definitionsto address fundamental concepts  needed to understand the new paradigm for data applications, collectively known as Big Data, and the  analytic processes collectively known as data science.Note that this definition contains the interplay between the characteristics of the data and the need for a  system architecture that can scale to achieve the needed performance and cost efficiency.Data science can be understood as the activities happening in the processing layer of the system  architecture, against data stored in the data layer,in order to extract knowledge from the raw data.In the new Big Data paradigm, it is implied that data sampling from the overall data  population is no longer necessary since the Big Data system can theoretically process all the data without  loss of performance.As open data(data available to others) and linked data(data that is connected to other data) become the  norm, it is increasingly important to have information about how data was collected, transmitted, and  processed.Dataset characteristics can refer to the data itself, or data at rest, while  characteristics of the data that is traversing a network or temporarily residing in computer memory to be  read or updated is referred to as data in motion, which is discussed in Section 3.4.The second characteristic of data at rest is the increasing need to use a variety of data, meaning the data  represents a number of data domains and a number of data types.Data science is the empirical synthesis of actionable knowledge from raw data through the complete data  life cycle process.
--------- Summary by LexRank algorithm --------- 
The NIST Big Data Public Working Group (NBD-PWG) Definitions and Taxonomy Subgroup prepared  this NIST Big Data Interoperability Framework: Volume 1, Definitionsto address fundamental concepts  needed to understand the new paradigm for data applications, collectively known as Big Data, and the  analytic processes collectively known as data science.Another concept of Big Data is often referred to as moving the processing to the data, not the data to the  processing.Additional concepts used in reference to the term Big Data refer to changes in analytics, which will be  discussed in Section 2.2.For data-intensive applications, all of  these skill groups are needed to distribute both the data and the computation across systems of resources  working in parallel.These analytics concepts are discussed  further in Section 3.4.As such, data science includes all of analytics,  but analytics does not include all of data science.Map/Reduce is discussed further in Volume 6.New  Big Data engineering technologies change the types of analytics that are possible, but do not result in  completely new types of analytics.

 ************ Summaries for file nist_NISTSP1500-2.txt. ************ 

--------- Summary by usual TextRank algorithm --------- 
To advance progress in Big Data, the NIST Big Data Public Working Group (NBD-PWG) is  working to develop consensus on important, fundamentalconcepts related to Big Data.The NIST Big Data Public Working Group (NBD-PWG) Definitions and Taxonomy Subgroup prepared  this NIST Big Data Interoperability Framework: Volume 1, Definitionsto address fundamental concepts  needed to understand the new paradigm for data applications, collectively known as Big Data, and the  analytic processes collectively known as data science.While Big Data has been defined in a myriad of  ways, the shift to a Big Data paradigm occurs when the scale of the data leads to the need for a cluster of  computing and storage resources to provide cost-effective data management.For descriptions of the future of Big Data and opportunities to use Big Data technologies, the reader is  referred to the NIST Big Data Interoperability Framework: Volume 7, Standards Roadmap.A number of new business models have been created for Big Data systems, including Data as a Service  (DaaS), where a business provides the Big Data Application Provider role as a service to other actors.In  this case, the business model is to process data received from a Data Provider and provide the transformed  data to the contracted Data Consumer.Once the data is within the local system, requests to retrieve the needed data will be made by the Big Data  Application Provider and routed to the Big Data Framework Provider.Understanding what characteristics have changed with Big Data can best be done by  examining the data scales of data elements, of related data elements grouped into a record that represents  a specific entity or event, ofrecords collected into a dataset, and of multiple datasetsall in turn, as  shown in Figure 10.
--------- Summary by LSA algorithm --------- 
The Information Technology Laboratory (ITL) at NIST promotes the U.S. economy and public welfare by  providing technical leadership for the Nation’s measurement and standards infrastructure.This document reports on ITL’s research, guidance, and outreach efforts in  Information Technology and its collaborative activities with industry, government, and academic  organizations.The NIST Big Data Interoperability Framework consists of seven volumes, each of which addresses a  specific key topic, resulting from the work of the NBD-PWG.There is no expectation of completeness in the components; the intent is to  provide enough context to understand the specific areas that have changed because of the new Big Data  paradigm.Finally, to  understand how these systems are organized and integrated to meet users’ needs, the reader is referred to  NIST Big Data Interoperability Framework:Volume 3, Use Cases and General Requirements.Examples of actors include the following:  Sensors,  Applications, Software agents, Individuals,  Organizations,  Hardware resources,  Service abstractions.In Section  2 of this document, a taxonomy was presented for the NBDRA, which is described in NIST Big Data  Interoperability Framework: Volume 6, Reference Architecture.This document constitutes a first  presentation of these descriptions, and future enhancements should provide additional understanding of  what is new in Big Data and in specific technology implementations.
--------- Summary by Kullback–Leibler algorithm --------- 
NIST Big Data Interoperability  Framework:  Volume 2, Big Data Taxonomies.The NIST Big Data Public Working Group (NBD-PWG) Definitions and Taxonomy Subgroup prepared  this NIST Big Data Interoperability Framework: Volume 1, Definitionsto address fundamental concepts  needed to understand the new paradigm for data applications, collectively known as Big Data, and the  analytic processes collectively known as data science.The grouping-based description presents data elements, data records, datasets, and multiple datasets.In  this case, the business model is to process data received from a Data Provider and provide the transformed  data to the contracted Data Consumer.Choosing the data fields that are used to decide how to distribute the data across multiple nodes  will depend on the organization’s data analysis needs,and on the ability to use those fields to distribute  the data evenly across resources.Aspects of the data  sources activity include both online and offline sources.New forms of sensors are now providing not only  a number of sources of data, but also data in large quantities.Data elements are  understood by their data type and additional contextual data, or metadata, which provides history or  additional understanding about the data.
--------- Summary by LexRank algorithm --------- 
The NIST Big Data Public Working Group (NBD-PWG) Definitions and Taxonomy Subgroup prepared  this NIST Big Data Interoperability Framework: Volume 1, Definitionsto address fundamental concepts  needed to understand the new paradigm for data applications, collectively known as Big Data, and the  analytic processes collectively known as data science.This taxonomy provides the terminology and  definitions for the components of technical systems that implement technologies for Big Data.In the  NBDRA presented in NIST Big Data Interoperability Framework Volume 6: Reference Architecture,  there are two roles that span the activities within the other roles: Management, and Security and Privacy.Finally, to  understand how these systems are organized and integrated to meet users’ needs, the reader is referred to  NIST Big Data Interoperability Framework:Volume 3, Use Cases and General Requirements.The System Orchestrator role includes defining and integrating the required data application activities  into an operational vertical system.Big Data systems potentially interact with processes and data being provided by other organizations,  requiring more detailed governance and monitoring between the components of the overall system.In Section  2 of this document, a taxonomy was presented for the NBDRA, which is described in NIST Big Data  Interoperability Framework: Volume 6, Reference Architecture.This taxonomy is a first attempt at providing a hierarchy for categorizing the new  components and activities of Big Data systems.