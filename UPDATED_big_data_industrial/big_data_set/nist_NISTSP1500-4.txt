NIST Special Publication 1500-4. NIST Big Data Interoperability  Framework:  Volume 4, Security and Privacy. The Information Technology Laboratory (ITL) at NIST promotes the U.S. economy and public welfare by  providing technical leadership for the Nation’s measurement and standards infrastructure. ITL develops  tests, test methods, reference data, proof of concept implementations, and technical analyses to advance  the development and productive use of information technology. ITL’s responsibilities include the  development of management, administrative, technical, and physical standards and guidelines for the  cost-effective security and privacy of other than national security-related information in federal  information systems. This document reports on ITL’s research, guidance, and outreach efforts in  Information Technology and its collaborative activities with industry, government, and academic  organizations. Big Data is a term used to describe the large amountof data in the networked, digitized, sensor-laden,  information-driven world. While opportunities exist with Big Data, the data can overwhelm traditional  technical approaches and the growth of data is outpacing scientific and technological advances in data  analytics. To advance progress in Big Data, the NIST Big Data Public Working Group (NBD-PWG) is  working to develop consensus on important, fundamentalconcepts related to Big Data. The results are  reported in the NIST Big Data Interoperability Frameworkseries of volumes. This volume, Volume 1,  contains a definition of Big Data and related terms necessary to lay the groundwork for discussions  surrounding Big Data.  The NIST Big Data Public Working Group (NBD-PWG) Definitions and Taxonomy Subgroup prepared  this NIST Big Data Interoperability Framework: Volume 1, Definitionsto address fundamental concepts  needed to understand the new paradigm for data applications, collectively known as Big Data, and the  analytic processes collectively known as data science. While Big Data has been defined in a myriad of  ways, the shift to a Big Data paradigm occurs when the scale of the data leads to the need for a cluster of  computing and storage resources to provide cost-effective data management. Data science combines  various technologies, techniques, and theories from various fields, mostly related to computer science and  statistics, to obtain actionable knowledge from data. This report seeks to clarify the underlying concepts  of Big Data and data science to enhance communication among Big Data producers and consumers. By  defining concepts related to Big Data and data science, a common terminology can be used among Big  Data practitioners.  The NIST Big Data Interoperability Framework consists of seven volumes, each of which addresses a  specific key topic, resulting from the work of the NBD-PWG. The seven volumes are as follows:  Definitions,  Taxonomies,  Use Cases and General Requirements,  Security and Privacy, Architectures White Paper Survey, Reference Architecture,  Standards Roadmap.  There is broad agreement among commercial, academic, and government leaders about the remarkable  potential of Big Data to spark innovation, fuel commerce, and drive progress. Big Data is the common  term used to describe the deluge of data in today’s networked, digitized, sensor-laden, and informationdriven world. The availability of vast data resources carries the potential to answer questions previously  out of reach, including the following:  How can a potential pandemic reliably be detected early enough to intervene?  Can new materials with advanced properties be predicted before these materials have ever been  synthesized?  How can the current advantage of the attackerover the defender in guarding against cyber­ security threats be reversed?  There is also broad agreement on the ability of Big Data to overwhelm traditional approaches. The growth  rates for data volumes, speeds, and complexity are outpacing scientific and technological advances in data  analytics, management, transport, and data user spheres.  Despite widespread agreement on the inherent opportunities and current limitations of Big Data, a lack of  consensus on some important fundamental questions continues to confuse potential users and stymie  progress. These questions include the following:  What attributes define Big Data solutions?  How is Big Data different from traditional dataenvironments and related applications?  What are the essential characteristics of Big Data environments?  How do these environments integrate withcurrently deployed architectures?  What are the central scientific, technological, and standardization challenges that need to be  addressed to accelerate the deployment of robust Big Data solutions?  Within this context, on March 29, 2012, the White House announced the Big Data Research and  Development Initiative. The initiative’s goals include helping to accelerate the pace of discovery in  science and engineering, strengthening national security, and transforming teaching and learning by  improving the ability to extract knowledge and insights from large and complex collections of digital  data.  Six federal departments and their agencies announcedmore than $200 million in commitments spread  across more than 80 projects, which aim to significantly improve the tools and techniques needed to  access, organize, and draw conclusions from huge volumes of digital data. The initiative also challenged  industry, research universities, and nonprofits to join with the federal government to make the most of the  opportunities created by Big Data.  Motivated by the White House initiative and public suggestions, the National Institute of Standards and  Technology (NIST) has accepted the challenge to stimulate collaboration among industry professionals to  further the secure and effective adoption of Big Data. As one result of NIST’s Cloud and Big Data Forum  held on January 15–17, 2013, there was strong encouragement for NIST to create a public working group  for the development of a Big Data Interoperability Framework. Forum participants noted that this  roadmap should define and prioritize Big Data requirements, including interoperability, portability,  reusability, extensibility, data usage, analytics, and technology infrastructure. In doing so, the roadmap  would accelerate the adoption of the most secure and effective Big Data techniques and technology.  The NBD-PWG Security and Privacy Subgroup explored various facets of Big Data security and privacy  to develop this document. The major steps involved in this effort included:  Announce that the NBD-PWG Security and Privacy Subgroup is open to the public in order to  attract and solicit a wide array of subject matter experts and stakeholders in government, industry,  and academia,  Identify use cases specific to Big Data security and privacy,  Develop a detailed security and privacy taxonomy,  Expand the security and privacy fabric of the NBDRA and identify specific topics related to  NBDRA components, and  Begin mapping of identified security and privacy use cases to the NBDRA.  This report is a compilation of contributions from the PWG. Since this is a community effort, there are  several topics covered that are related to security and privacy. While an effort has been made to connect  the topics, gaps may come to light that could be addressed in Version 2 of this document.  The NBD-PWG Security and Privacy Subgroup plans to further develop several topics for the subsequent  version (i.e., Version 2) of this document. These topics include the following:  Examining closely other existing templates in literature: The templates may be adapted to the  Big Data security and privacy fabric to address gaps and to bridge the efforts of this Subgroup  with the work of others,  Further developing the security and privacy taxonomy,  Enhancing the connection between the security and privacy taxonomy and the NBDRA  components,  Developing the connection between the security and privacy fabric and the NBDRA,  Expanding the privacy discussion within the scope of this volume,  Exploring governance, risk management, data ownership, and valuation with respect to Big Data  ecosystem, with a focus on security and privacy,  Mapping the identified security and privacy use cases to the NBDRA,  Contextualizing the content of Appendix B in the NBDRA, and  Exploring privacy in actionable terms based on frameworks such as those described in NISTIR  with respect to the NBDRA.  Further topics and direction may be added, as warranted, based on future input and contributions to the  Subgroup, including those received during the public comment period. Security and privacy measures are becoming ever more important with the increase of Big Data  generation and utilization and increasingly public nature of datastorage and availability.  The importance of security and privacy measures is increasing along with the growth in the generation,  access, and utilization of Big Data. Data generation is expected to double every two years to about 40,000  exabytes in 2020. It is estimated that over one-third of the data in 2020 could be valuable if analyzed. 3  Less than a third of data needed protection in 2010, but more than 40 percent of data will need protection  in 2020. 4  Security and privacy measures for Big Data involve a different approach than traditional systems. Big  Data is increasingly stored on public cloud infrastructure built by employing various hardware, operating  systems, and analytical software. Traditional security approaches usually addressed small-scale systems  holding static data on firewalled and semi-isolated networks. The surge in streaming cloud technology  necessitates extremely rapid responses to security issues and threats. 5  Big Data system representations that rely on concepts of actors and roles present a different facet to  security and privacy. The Big Data systems should be adapted to the emerging Big Data landscape, which  is embodied in many commercial and open source access control frameworks. These security approaches will likely persist for some time and may evolve with the emerging Big Data landscape. Appendix C  considers actors and roles with respect to Big Data security and privacy.  Big Data is increasingly generated and used across diverse industries such as healthcare, drug discovery,  finance, insurance, and marketing of consumer-packaged goods. Effective communication across these  diverse industries will require standardization of the terms related to security and privacy. The NBD­ PWG Security and Privacy Subgroup aims to encourage participation in the global Big Data discussion  with due recognition to the complex and difficult security and privacy requirements particular to Big  Data.  There is a large body of work in security and privacy spanning decades of academic study and  commercial solutions. While much of that work is notconceptually distinct from Big Data, it may have  been produced using different assumptions. One of the primary objectives of this document is to  understand how Big Data security and privacy requirements arise out of the defining characteristics of  Big Data, and how these requirements are differentiated from traditional security and privacy  requirements.  The following list is a representative—though not exhaustive—list of differences between what is new for  Big Data and the requirements that informed previous big system security and privacy.  Big Data may be gathered from diverse end points. Actors include more types than just  traditional providers and consumers—data owners, such as mobile users and social network users,  are primary actors in Big Data. Devices that ingest data streams for physically distinct data  consumers may also be actors. This alone is not new, but the mix of human and device types is on  a scale that is unprecedented. The resulting combination of threat vectors and potential protection  mechanisms to mitigate them is new.  Data aggregation and dissemination must be secured inside the context of a formal,  understandable framework.The availability of data and transparency of its current and past use  by data consumers is an important aspect ofBig Data. However, Big Data systems may be  operational outside formal, readily understood frameworks, such as those designed by a single  team of architects with a clearly defined setof objectives. In some settings, where such  frameworks are absent or have been unsystematically composed, there may be a need for public  or walled garden portals and ombudsman-like roles for data at rest. These system combinations  and unforeseen combinations call for a renewed Big Data framework. Data search and selection can lead to privacy or security policy concerns. There is a lack of  systematic understanding of the capabilities that should be provided by a data provider in this  respect. A combination of well-educated users, well-educated architects, and system protections  may be needed, as well as excluding databases orlimiting queries that may be foreseen as  enabling re-identification. If a key feature of Big Data is, as one analyst called it, “the ability to  derive differentiated insights from advanced analytics on data at any scale,” the search and  selection aspects of analytics will accentuate security and privacy concerns. Privacy-preserving mechanisms are needed for Big Data, such as for Personally Identifiable  Information (PII). Because there may be disparate, potentially unanticipated processing steps  between the data owner, provider,and data consumer, the privacy and integrity of data coming  from end points should be protected at every stage. End-to-end information assurance practices  for Big Data are not dissimilar from other systems but must be designed on a larger scale.  Big Data is pushing beyond traditional definitions for information trust, openness, and  responsibility.Governance, previously consigned to static roles and typically employed in larger  organizations, is becoming an increasingly important intrinsic design consideration for Big Data  systems. Legacy security solutions need to be retargeted to the infrastructural shift due to Big Data.  Legacy security solutions address infrastructural security concerns that still persist in Big Data,  such as authentication, access control and authorization. These solutions need to be retargeted to  the underlying Big Data High Performance Computing (HPC) resources or completely replaced.  Oftentimes, such resources can face the public domain, and thus necessitate vigilant security  methods to prevent adversarial manipulation and preserve integrity of operations.  Information assurance and disaster recovery for Big Data Systems may require unique and  emergent practices.Because of its extreme scalability, Big Data presents challenges for  information assurance (IA) and disaster recovery (DR) practices that were not previously  addressed in a systematic way. Traditional backup methods may be impractical for Big Data  systems. In addition, test, verification, and provenance assurance for Big Data replicas may not  complete in time to meet temporal requirements that were readily accommodated in smaller  systems.  Big Data creates potential targets of increased value.The effort required to consummate  system attacks will be scaled to meet the opportunity value. Big Data systems will present  concentrated, high-value targets to adversaries. As Big Data becomes ubiquitous, such targets are  becoming more numerousa new information technology (IT) scenario in itself.  Risks have increased for de-anonymization and transfer of PII without consent traceability.  Security and privacy can be compromised through unintentional lapses or malicious attacks on  data integrity. Managing data integrity for Big Data presents additional challenges related to all  the Big Data characteristics, but especially for PII. While there are technologies available to  develop methods for de-identification, some experts caution that equally powerful methods can  leverage Big Data to re-identify personal information. For example, the availability of  unanticipated datasets could make re-identification possible. Even when technology is able to  preserve privacy, proper consent and use may not follow the path of the data through various  custodians. Because of the broad collection and set of uses of big data, consent for collection is  much less likely to be sufficient and should be augmented with technical and legal controls to  provide auditability and accountability for use. Emerging Risks in Open Data and Big Science. Data identification, metadata tagging,  aggregation, and segmentation—widely anticipated for data science and open datasets—if not  properly managed, may have degraded veracity because they are derived and not primary  information sources. Retractions of peer-reviewed research due to inappropriate data  interpretations may become more commonplace as researchers leverage third-party Big Data.  Variety, volume, velocity, and variability are key characteristics of Big Data and commonly referred to as  the Vs of Big Data. Where appropriate, these characteristics shaped discussions within the NBD-PWG  Security and Privacy Subgroup. While the Vs provide a useful shorthand description, used in the public  discourse about Big Data, there are other important characteristics of Big Data that affect security and  privacy, such as veracity, validity, and volatility. These elements are discussed below with respect to their  impact on Big Data security and privacy.  Variety describes the organization of the data—whether the data is structured, semi-structured, or  unstructured. Retargeting traditional relational database security to non-relational databases has been a  challenge. These systems were not designed with security and privacy in mind, and these functions are  usually relegated to middleware. Traditional encryption technology also hindersorganization of data  based on semantics. The aim of standard encryption isto provide semantic security, which means that the  encryption of any value is indistinguishable from the encryption of any other value. Therefore, once encryption is applied, any organization of the data that depends on any property of the data values  themselves are rendered ineffective, whereas organization of the metadata, which may be unencrypted,  may still be effective.  An emergent phenomenon introduced by Big Data variety that has gained considerable importance is the  ability to infer identity from anonymized datasets by correlating with apparently innocuous public  databases. While several formal models to address privacy preserving data disclosure have been  proposed,  in practice, sensitive data is shared after sufficient removal of apparently unique identifiers,  and indirectly identifying information by the processes ofanonymization and aggregation. This is an ad  hoc process that is often based on empirical evidence  and has led to many instances of de­ anonymization in conjunction with publicly available data.  Although some laws/regulations recognize  only identifiers per se, laws such as HIPAA (the statistician provision), FERPA, and 45 CFR 46 recognize  that combinations of attributes, even if not the identifiers by themselves, can lead to actionable personal  identification, possibly in conjunction with external information.   Big Data veracity and validity encompass several sub-characteristics:  Provenance—or what some have called veracity in keeping with the V theme—is important for both data  quality and for protecting security and maintaining privacy policies. Big Data frequently moves across  individual boundaries to groups and communities of interest, and across state, national, and international  boundaries. Provenance addresses the problem of understanding the data’s original source, such as  through metadata, though the problem extends beyond metadata maintenance. Also as noted before, with  respect to privacy policy, additional context is needed to make responsibledecisions over collected  data—this may include the form of consent, intended use, temporal connotations (like Right to be  Forgotten) or broader context of collection. This could be considered a type of provenance broadly, but  goes beyond the range of provenance information typically collected in production information systems.  Various approaches have been tried, such as for glycoproteomics, but no clear guidelines yet exist.  A common understanding holds that provenance data ismetadata establishing pedigree and chain of  custody, including calibration, errors, missing data (e.g., time stamp, location, equipment serial number,  transaction number, and authority.)  Some experts consider the challenge of defining and maintaining metadata to be the overarching  principle, rather than provenance. The two concepts, though, are clearly interrelated.  Veracity(in some circles also called Provenance, though the two terms are not identical) also  encompasses information assurance for the methods through which information was collected. For  example, when sensors are used, traceability, calibration, version, sampling, and device configuration is  needed.  Curation is an integral concept which binds veracity and provenance to principles of governance as well  as to data quality assurance. Curation, for example, mayimprove raw data by fixing errors, filling in gaps,  modeling, calibrating values, and ordering data collection.  Furthermore, there is a central and broadly recognized privacy principle, incorporated in many privacy  frameworks (e.g., the OECD principles, EU data protection directive, FTC fair information practices) that  data subjects must be able to view and correct information collected about them in a database.  Validityrefers to the accuracy and correctness of data. Traditionally, this is referred to data quality. In the  Big Data security scenario, validity refers to a host of assumptions about data from which analytics are  being applied. For example, continuous and discrete measurements have different properties. The field  “gender” can be coded as 1=Male, 2=Female, but 1.5 does not mean halfway between male and female.  In the absence of such constraints, an analytical tool can make inappropriate conclusions. There are many  types of validity whose constraints are far more complex. By definition, Big Data allows for aggregation  and collection across disparate datasets in ways not envisioned by system designers.  Several examples of “invalid” uses for Big Data have been cited. Click fraud, conducted on a Big Data  scale, but which can be detected using Big Data techniques, has been cited as the cause of perhaps $11  billion in wasted advertisement spending. A software executive listed seven different types of online ad  fraud, including nonhuman generated impressions, nonhuman generated clicks, hidden ads,  misrepresented sources, all-advertising sites, maliciousad injections, and policy-violating content such as  pornography or privacy violations. Each of these can be conducted at Big Data scale and may require  Big Data solutions to detect and combat.  Despite initial enthusiasm, some trend-producing applications that use social media to predict the  incidence of flu have been called into question. A study by Lazer et al. suggested that one application  overestimated the prevalence of flu for 100 of 108 weeks studied. Careless interpretation of social media  is possible when attempts are made to characterize oreven predict consumer behavior using imprecise  meanings and intentions for “like” and “follow.”  These examples show that what passes for “valid” Big Data can be innocuously lost in translation,  interpretation, or intentionally corrupted to malicious intent.  Many Big Data systems will be designed using cloud architectures. Any strategy to achieve proper access  control and security risk management within a Big Data cloud ecosystem enterprise architecture must  address the complexities associated with cloud-specific security requirements triggered by cloud  characteristics, including, but not limited to, the following:  Broad network access,  Decreased visibility and control by consumer,  Dynamic system boundaries and commingled roles and responsibilities between consumers and  providers,  Multi-tenancy, Data residency,  Measured service  Order-of-magnitude increases in scale (on demand), dynamics (elasticity and cost optimization),  and complexity (automation and virtualization). These cloud computing characteristics often present different security risks to an organization than the  traditional IT solutions, altering the organization’s security posture.  To preserve security when migrating data to the cloud, organizations need to identify all cloud-specific,  risk-adjusted security controls or components in advance. It may be necessary in some situations to  request from the cloud service providers through contractual means and service-level agreements that all  require security components and controls tobe fully and accurately implemented.  A further discussion of internal security considerations within cloud ecosystems can be found in  Appendix B. Future versions of this document will contextualize the content of Appendix B in the  NBDRA.  There are significant Big Data challenges in science and engineering. Many of these are described in the  use cases in NIST Big Data Interoperability Framework: Volume 3, Use Cases and General  Requirements. However, the primary focus of these use cases was on science and engineering  applications, and therefore security and privacy impacts on system architecture were not highlighted.  Consequently, a different set of use cases, presented in this document, was developed specifically to  discover security and privacy issues. Some of these use cases represent inactive or legacy applications,  but were selected to demonstrate characteristic security/privacy design patterns.  The use cases selected for security and privacy are presented in the following subsections. The use cases  included are grouped to organize this presentation, as follows: retail/marketing, healthcare, cybersecurity,  government, industrial, aviation, and transportation. However, these groups do not represent the entire  spectrum of industries affected by Big Data security and privacy.  The use cases were collected when the reference architecture was not mature. The use cases were  collected from BDWG members to identify representative security and privacy scenarios thought to be  suitably classified as particular to Big Data. An effort was made to map the use cases to the NBDRA. In  Version 2, additional mapping of the use cases to the NBDRA and taxonomy will be developed. Parts of  this document were developed in parallel, and the connections will be strengthened in Version 2.  Scenario Description:Consumers, with the help of smart devices, have become very conscious of price,  convenience, and access before they decide on a purchase. Content owners license data for use by  consumers through presentation portals, such as Netflix, iTunes, and others.  Comparative pricing from different retailers, store location and/or delivery options, and crowd-sourced  rating have become common factors for selection. Tocompete, retailers are keeping a close watch on  consumer locations, interests, and spending patterns to dynamically create marketing strategies and sell  products that consumers do not yet know they want.  Current Security and Privacy Issues/Practices:Individual data is collected by several means, including  smartphone GPS (global positioning system) or location,browser use, social media, and applications  (apps) on smart devices.  A candidate set of topics from the Cloud SecurityAlliance Big Data Working Group (CSA BDWG)  article, Top Ten Challenges in Big Data Security and Privacy Challenges, was used in developing these  security and privacy taxonomies. Candidate topics and related materialused in preparing this section are  provided for reference in Appendix A.  A taxonomy for Big Data security and privacy should encompass the aims of existing useful taxonomies.  While many concepts surrounding security and privacy exist, the objective in the taxonomies contained  herein is to highlight and refine new or emerging principles specific to Big Data.  The following subsections present an overview of each security and privacy taxonomy, along with lists of  topics encompassed by the taxonomy elements. These listsare the results of preliminary discussions of  the Subgroup and may be developed further in Version 2. As noted earlier, Version 1 focuses  predominantly on security and security-related privacy risks (i.e. risks that result from unauthorized  access to personally identifiable information). Privacy risks that may result from the processing of  information about individuals and how the taxonomy may account for such considerations will be  explored in greater detail in future versions.   The conceptual security and privacy taxonomy, presented in Figure 2, contains four main groups: data  confidentiality; data provenance; system health; and public policy, social, and cross-organizational topics.  The first three topics broadly correspond with the traditional classification of confidentiality, integrity,  and availability (CIA), reoriented toparallel Big Data considerations. The following set of topics is drawn from an Association for Computing Machinery (ACM) grouping.  Each of these topics has Big Data security and privacy dimensions that could affect how a fabric overlay  is implemented for a specific Big Data project. For instance, a medical devices project might need to  address human safety risks, whereas a banking project would be concerned withdifferent regulations  applying to Big Data crossing borders. Further work to develop these concepts for Big Data is anticipated  by the Subgroup.   Current practice for securing Big Data systems is diverse, employing widely disparate approaches that  often are not part of a unified conceptual framework. The elements of the operational taxonomy, shown in  Figure 3, represent groupings of practical methodologies.These elements are classified as “operational”  because they address specific vulnerabilities or risk management challenges to the operation of Big Data  systems. At this point in the standards development process, these methodologies have not been  incorporated as part of a cohesive security fabric. They are potentially valuable checklist-style elements  that can solve specific security or privacy needs. Future work must better integrate these methodologies  with risk management guidelines developed by others(e.g., NIST Special Publication 800-37 Revision 1,  Guide for Applying the Risk Management Framework to Federal Information Systems , draft NIST  Internal Report 8062, Privacy Risk Management for Federal Information Systems , and COBIT Risk IT  Framework ).  In the proposed operational taxonomy, broad considerations of the conceptual taxonomy appear as  recurring features. For example, confidentiality of communications can apply to governance of data at rest  and access management, but it is also part of a security metadata model.  The operational taxonomy will overlap with small data taxonomies while drawing attention to specific  issues with Big Data.  Discussions of Big Data security and privacy should be accessible to a diverse audience both within an  organization and across supply chains. Access should include individuals who specialize in cryptography,  security, compliance, or IT. In addition, the ideal audience includes domain experts and organization  decision makers who understand the costs and impact of these controls. Ideally, written guidelines setting  forth policy and compliance for Big Data securityand privacy would be prefaced by additional  information that would help specialists find the content relevant to them. The specialists could then  provide feedback on those sections.  Organizations typically contain diverse roles and workflows for participating in a Big Data ecosystem.  Therefore, this document proposes a pattern to helpidentify the “axis” of an individual’s roles and  responsibilities, as well as classify the security controlsin a similar manner to make these more accessible  to each class.  Data governance is a fundamental element in the management of data and data systems. Data governance  refers to administering, or formalizing, discipline (e.g., behavior patterns) around the management of  data. Risk management involves the evaluation of positive and negative risks resulting from the handling  of Big Data. Compliance encompasses adherence to laws, regulations, protocols, and other guiding rules  for operations related to Big Data. Typically, governance, risk management, and compliance (GRC) is a  function that draws participation from multiple areas of the organization, such as legal, human resources  (HR), IT, and compliance. In some industries and agencies, there may be a strong focus on compliance,  often in isolation from disciplines.  Professionals working in GRC tend to have similar backgrounds, share a common terminology, and  employ similar processes and workflows, which typically influence other organizations within the  corresponding vertical market or sector.  Within an organization, GRC professionals aim to protect the organization from negative outcomes that  might arise from loss of intellectual property, liability due to actions by individuals within the  organization, and compliance risks specific to its vertical market.  In larger enterprises and government agencies, GRC professionals are usually assigned to legal,  marketing, or accounting departments or staff positionsconnected to the CIO. Internal and external  auditors are often involved.  Smaller organizations may create, own, or process Big Data, yet may not have GRC systems and  practices in place, due to the newness of the Big Data scenario to the organization, a lack of resources, or  other factors specific to small organizations. Prior to Big Data, GRC roles in smaller organizations  received little attention.  A one-person company can easily construct a Big Dataapplication and inherit numerous unanticipated  related GRC responsibilities. This is a new GRC scenario.  A security and privacy fabric entails additional data and process workflow in support of GRC, which is  most likely under the control of the System Orchestrator component of the NBDRA, as explained in  Section 5.  Information workers are individuals and groups who work on the generation, transformation, and  consumption of content. Due to the nascent natureof the technologies and related businesses in which  they work, they tend to use common terms at a technical level within a specialty. However, their roles and  responsibilities and the related workflows do not always align across organizational boundaries. For  example, a data scientist has deep specialization in the content and its transformation, but may not focus  on security or privacy until it adds effort, cost, risk, or compliance responsibilities to the process of  accessing domain-specific data or analytical tools.  Information workers may serve as data curators. Some may be research librarians, operate in quality  management roles, or be involved in information management roles such as content editing, search  indexing, or performing forensic duties as part of legal proceedings.  The next sections cover the four components of the conceptual taxonomy: data confidentiality, data  provenance, system health, and public policy, social and cross-organizational topics. To leverage these  three axes and to facilitate collaboration and education,a stakeholder can be defined as an individual or  group within an organization who is directly affected by the selection and deployment of a Big Data  solution. A ratifier is defined as an individual or group within an organization who is tasked with  assessing the candidate solution before it is selected and deployed. For example, a third-party security  consultant may be deployed by an organization as a ratifier, and an internal security specialist with an  organization’s IT department might serve as both a ratifier and a stakeholder if tasked with ongoing  monitoring, maintenance, and audits of the security.  The upcoming sections also explore potential gaps that would be of interest to the anticipated  stakeholders and ratifiers who resideon these three new conceptual axes.  IT specialists who address cryptography should understand the relevant definitions, threat models,  assumptions, security guarantees, and core algorithms and protocols. These individuals will likely be  ratifiers, rather than stakeholders. IT specialists who address end-to-end security should have an  abbreviated view of the cryptography, as well as a deep understanding of how the cryptography would be  integrated into their existing security infrastructures and controls.  GRC should reconcile the vertical requirements (e.g., HIPAA requirements related to EHRs) and the  assessments by the ratifiers that address cryptography and security. GRC managers would in turn be  ratifiers to communicate their interpretation of the needs of their vertical. Persons in these roles also serve  as stakeholders due to their participation in internal and external audits and other workflows.  A feature of Big Data systems is that data is bought and sold as a valuable asset. That Google Search is  free relies on users giving up information about their search terms on a Big Data scale. Google and  Facebook can choose to repackage and syndicate that information for use by others for a fee.  Similar to service syndication, a data ecosystem is most valuable if any participant can have multiple  roles, which could include supplying, transforming, orconsuming Big Data. Therefore, a need exists to  consider what types of data syndication models should be enabled; again, information workers and IT  professionals are candidate ratifiers and stakeholders. For some domains, more complex models may be  required to accommodate PII, provenance, and governance. Syndication involves transfer of risk and  responsibility for security and privacy.  Security and privacy considerations are a fundamental aspect of the NBDRA. Using the material gathered  for this volume and extensive brainstorming among the NBD-PWG Security and Privacy Subgroup  members and others, the following proposal for a security and privacy fabric was developed.  Security and Privacy Fabric:Security and privacy considerations form a fundamental aspect of the  NBDRA. This is geometrically depicted in Figure 4 by the Security and Privacy Fabric surrounding the  five main components, since all components are affected by security and privacy considerations. Thus,  the role of security and privacy is correctly depicted in relation to the components but does not expand  into finer details, which may be more accurate but are best relegated to a more detailed security and  privacy reference architecture. The Data Provider and Data Consumer are included in the Security and  Privacy Fabric since, at the least, they should agree on the security protocols and mechanisms in place.  The Security and Privacy Fabric is an approximate representation that alludes to the intricate  interconnected nature and ubiquity of security and privacy throughout the NBDRA.  This pervasive dimension is depicted in Figure 4 by the presence of the security and privacy fabric  surrounding all of the functional components. NBD-PWG decided to include the Data Provider and Data  Consumer as well as the Big Data Application and Framework Providers in the Security and Privacy  Fabric because these entities should agree on the security protocols and mechanisms in place. The NIST  Big Data Interoperability Framework: Volume 6, Reference Architecturedocument discusses in detail the  other components of the NBDRA.  At this time, explanations as to how the proposed fabric concept is implemented across each NBDRA  component are cursorymore suggestive than prescriptive. However, it is believed that, in time, a  template will evolve and form a sound basis for more detailed iterations.  Big Data security and privacy should leverage existing standards and practices. In the privacy arena, a  systems approach that considers privacy throughout the process is a useful guideline to consider when  adapting security and privacy practices to Big Data scenarios. The Organization for the Advancement of  Structured Information Standards (OASIS) PrivacyManagement Reference Model (PMRM), consisting  of seven foundational principles, provides appropriate basic guidance for Big System architects.   When working with any personal data, privacy should be an integral element in the design of a Big Data  system.  Other privacy engineering frameworks, including the model presented in draft NISTIR 8062, Privacy  Risk Management for Federal Information Systems, are also under consideration.   Related principles include identity management frameworks such as proposed in the National Strategy for  Trusted Identities in Cyberspace (NSTIC)  and considered in the NIST Cloud Computing Security  Reference Architecture.  Aspects of identity management that contribute to a security and privacy fabric  will be addressed in future versions of this document.  Big Data frameworks can also be used for strengthening security. Big Data analytics can be used for  detecting privacy breaches through security intelligence, event detection, and forensics.