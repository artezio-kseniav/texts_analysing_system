Customer Cloud Architecture for Big Data and Analytics, Version 1.1. Executive Overview. Using analytics reveals patterns, trends and associations in data that help an organization understand the behavior of the people and systems that drive its operation. Big data technology increases the  amount and variety of data that can be processed by analytics, providing a foundation for visualizations  and insights that can significantly improve business operations.  This paper considers how harnessing cloud architectures can further change the economics and  development lifecycle of these capabilities. It describes vendor neutral best practices for hosting big  data and analytics solutions (or just “analytics solutions”) using cloud computing. The architectural  elements described in this document will help you understand the components for leveraging various cloud deployment models.  The primary drivers for deploying analytics solutions on cloud include: 1.  Low upfront cost of infrastructure and a reduction in the skills needed to get started. 2.  Elastic data and processing resources that grow and shrink with demand, reducing the need to  maintain capacity for the maximum workload. 3.  Mitigation against limited internal capability for meeting information governance, compliance  and security requirements.  4.  Applying more processing resources to existing data sources. 5.  Building solutions faster because it enables try and buy, rapid prototyping and shorter  procurement processes. Cloud deployments offer a choice of private, public and hybrid architectures. Private cloud employs inhouse data and processing components running behind corporate firewalls. Public cloud offers services  over the internet with data and computing resources available on publicly assessable servers. Hybrid  environments have a mixture of components running as both in-house and public services. It is important to have this choice of cloud deployment because location is one of the first architectural  decisions for an analytics cloud project. In particular, where should the data be located and where  should the analytics processing be located relative to the location of the data? Legal and regulatory  requirements may also impact where data can be located since many countries have data sovereignty  laws that prevent data about individuals, finances and intellectual property from moving across country  borders. The choice of cloud architectures allows compute components to be moved near data to optimize  processing when data volume and bandwidth limitations produce remote data bottlenecks. For example, consider an existing data set that is very large or highly volatile. It would be expensive to  move, so the analytics processing system may need to access this data from its current storage location. The amount of data that the analytics processing system needs would then determine whether the  analytics needs to be hosted with this data, or whether it can use APIs to retrieve the data it needs remotely. Much of the data that an organization might process with analytics could be generated by existing systems and the log files and related documents that accompany it. This may then be augmented with  data from third parties and new applications that are born on the cloud. Your cloud architecture needs  to make trade-offs for where the data is to be accumulated and processed. Analytics has a development lifecycle that also impacts where data is optimally located and managed. The first phase is the discovery and exploration of data. In this phase a catalog is used to discover  (locate) the data to analyse. They then access the data and explore its values using analytics tools. The  second phase is the development of the analytics model and finally, phase three is the deployment of  the analytics model into production. Each phase may run in the same cloud environment or be distributed in different locations. Typically  phase one and two (the discovery, exploration and analytical model development) is collated with a vast  collection of different types of data that has been harvested from its original sources. For phase three,  the completed analytical models may be deployed with this data, or placed close to where this data is  being generated, or where the resulting insight will be acted upon. Wherever the analytics model is  deployed, it is accompanied by new data collection processes that gather the results of the analytics so  they can be improved with another iteration of the analytics development lifecycle. The architecture of the analytics cloud solution may evolve as this solution matures. Preliminary and  proof-of-concept (POC) applications often start in public cloud environments where new resources can  be acquired and evaluated quickly with a minimal procurement process. Development and deployment  costs can then be estimated based on initial usage metrics. Development speed is generally enhanced  via continuous release methodologies and by leveraging Platform as a Service (PaaS) and Software as a  Service (SaaS). The cost model of paying for what you use, as you need it, is an attractive benefit of  public cloud deployment for evaluating new approaches.  Organizations needing on-premises data storage and processing cite data privacy, security and legal  constraints as chief motivations. Large data sets that cannot be moved and local operational  requirements are other factors that favor in-house provisioning. Private cloud deployment is a solution  that generally offers the most efficient access to this secure data while maintaining access to internally  shared software and analytics. Hybrid cloud deployment is emerging as a preferred choice of customers who want to balance their  requirements and costs. Critical data and processing remains in the enterprise data center, while other  resources are deployed in public cloud environments. Processing resources can be further optimized  with a hybrid topology that enables cloud analytics engines to work with on-premises data. This  leverages enhanced cloud software deployment and update cycles while keeping data inside the  firewall. Another benefit is the ability to develop applications on dedicated resource pools in a hybrid cloud  deployment that eliminates the need to compromise on configuration details like processors, GPUs,  memory, networking and even software licensing constraints. The resulting solution can be subsequently deployed to an Infrastructure as a Service (IaaS) cloud service that offers compute  capabilities matching the dedicated hardware environment which would be otherwise hosted on  premises. This feature is fast becoming a differentiator for cloud applications that need to hit the ground  running with the right configuration to meet real-world demand. Figure 1 shows the elements that may be needed for any big data analytics solution across three  domains: public networks, provider clouds, and enterprise networks. The public network and enterprise network domains contain data sources that feed the entire architecture. Data sources include traditional systems of record from the enterprise as well as emerging  sources from Internet of Things (IoT). The provider cloud uses data integration components and potentially streaming computing to capture this combined data into data repositories where analytics can be performed to deliver actionable  insights. These insights are used by users and enterprise applications as well as stored in data storage  systems. All of this is done in a secure and governed environment.  Results are delivered to users and applications using transformation and connectivity components which  provide secure messaging and translations into systems of engagement, enterprise data, and enterprise  applications. Big data architecture in a cloud computing environment has many similarities to a traditional data  center. Data is collected and staged by data integration so it can be prepared for intended consumers.  The data is collected from structured and non-structured data sources, including real-time data from  stream computing, and maintained in enterprise data stores. Common metadata and semantic  definitions are added to enterprise data repositories. Data repositories provide staging areas for the  different types of data. The data repositories provide the development environment for new analytics  models or enhancements of existing models. Once the models are executed, their outcome is provided  for use as actionable insight via information views into the data which are also exposed for ad-hoc  analysis by end users or other applications on the client premises. Data is transformed and augmented  as it moves through the processing chain. Information governance and security subsystems encompass each processing phase to ensure regulation and policies for all data are defined and enabled across the system. Compliance is tracked to  ensure controls are delivering expected results. Security covers all elements including generated data  and analytics. Users are broadly classified in two ways: enterprise and third party. Enterprise users access resources on  premises or via a secure Virtual Private Network (VPN). Data is available directly and through applications that provide reports and analytics. Transformation and connectivity gateways assist by  preparing information for use by enterprise applications as well as use on different devices, including  mobile, web browsers and desktop systems. Third party users gain access to the provider cloud or the  enterprise network via edge services that secure access to users with proper credentials. Access to other  resources may be further restricted as dictated by corporate policy. The remainder of this section describes the various components in detail. The public network contains elements that exist in the internet: data sources, users and the edge  services needed to access the provider cloud or enterprise network.  Data sources contain all of the external sources of data for the data analytics solutions that flow from  the internet. Users set up or use the results of the analytical system, and are typically part of the enterprise. Users  can be administrative type users, setting up the analytical processing system; analytical services users,  using the results of the analytical system; or enterprise users, invoking enterprise applications in the  analytical system. In the case of enterprise users, the access path may not go through the public internet  and may go directly to the analytical insights or enterprise applications.  When the data or user requests comes from the external internet, the flow may come through edge  services including DNS servers, Content Delivery Networks (CDNs), firewalls, and load balancers before  entering the cloud provider’s data integration or data streaming entry points. Data Sources There can be a number of different information sources in a typical big data system, some of which enterprises are just beginning to include in their data  analytics solutions. High velocity, volume, variety and data inconsistency  often kept many types of data from being used extensively. Big data tools  have enabled organizations to use this data; however, these typically run onpremises and can require substantial upfront investment. Cloud computing  helps mitigate that investment and the associated risk by providing big data  tools via a pay per use model. 