Big Data: Big today, normal tomorrow ITU-T Technology Watch Report November 2013. The rapid evolution of the telecommunication/information and communication technology (ICT)  environment requires related technology foresight and immediate action in order to propose ITU-T  standardization activities as early as possible.  ITU-T Technology Watch surveys the ICT landscape to capture new topics for standardization  activities. Technology Watch Reports assess new technologies with regard to existing standards  inside and outside ITU-T and their likely impact on future standardization. In early 2013 several European countries were rocked by a food scandal which uncovered a network  of fraud, mislabeling and sub-standard supply chain management.  This was not the first food scandal, and will surely not be the last. For restaurant chains with  thousands of branches and hundreds of suppliers worldwide, it is nearly impossible to monitor the  origin and quality of each ingredient. Data and sophisticated real-time analytics are means to  discover early (or, better yet, prevent) irregularities.  The events leading to the discovery and resolution of the scandal point to the promises and  challenges of data management for multiparty, multidimensional, international systems. Billions of  individual pieces of data are amassed each day, from sources including supplier data, delivery slips,  restaurant locations, employment records, DNA  records, data from Interpol’s database of  international criminals, and also customer complaints and user-generated content such as location  check-ins, messages, photos and videos on social media sites. But more data does not necessarily  translate into better information. Gleaning insight and knowledge requires ‘connecting the dots’ by  aggregating data and analyzing it to detect patterns and distill accurate, comprehensive, actionable  reports.  Big data – a composite term describing emerging technological capabilities in solving complex tasks –  has been hailed by industry analysts, business strategists and marketing pros as a new frontier for  innovation, competition and productivity. “Practically everything that deals with data or business  intelligence can be rebranded into the new gold rush”, and the hype around big data looks set to  match the stir created by cloud computing (see Figure 1) where existing offerings were rebranded as  ‘cloud-enabled’ overnight and whole organizations moved to the cloud.  Putting the buzz aside, big data motivates researchers from fields as diverse as physics, computer  science, genomics and economics – where it is seenas an opportunity to invent and investigate new  methods and algorithms capable of detecting usefulpatterns or correlations present in big chunks of  data. Analyzing more data in shorter spaces of time can lead to better, faster decisions in areas  spanning finance, health and research.  This Technology Watch report looks at different examples and applications associated with the big  data paradigm (section 2), identifies commonalities among them by describing their characteristics  (section 3), and highlights some of the technologiesenabling the upsurge of big data (section 4). As  with many emerging technologies, several challenges need to be identified (section 5) and addressed  to facilitate the adoption of big data solutions in a wider range of scenarios.Global standardization  can contribute to addressing such challenges and will help companies enter new markets, reduce  costs and increase efficiency. Big data standardization activities related to the ITU-T work programme  are described in the final section of this report. Data is critical in the healthcare industry where it documents the history and evolution of a patient’s  illness and care, giving healthcare providers the  tools they need to make informed treatment  decisions. With medical image archives growing by20 to 40 per cent annually, by 2015, an average  hospital is expected to be generating 665 terabytes of medical data each year. McKinsey  analysts  predict that, if large sets of medical data were routinely collected and electronic health records were  filled with high-resolution X-ray images, mammograms, 3D MRIs, 3D CT scans, etc., we could better  predict and cater to the healthcare needs of a population; which would not only drive gains in  efficiency and quality, but also cut the costs of healthcare dramatically. Applications of big data  analytics in the healthcare domainare as numerous as they are multifaceted, both in research and  practice, and below we highlight just a few.  Remote patient monitoring, an emerging market segment of machine-to-machine communications  (M2M), is proving a source of useful, quite literallylifesaving, information. People with diabetes, for  instance, are at risk of long-term complications such as blindness, kidney disease, heart disease and  stroke. Remote tracking of a glucometer (a blood sugar reader) helps monitor a patient’s compliance  with the recommended glucose level. Electronic health records are populated with data in near real  time. Time series of patient data can track a patient’s status, identify abnormalities and form the  basis of treatment decisions. More generally, exploiting remote patient monitoring systems for  chronically ill patients can reduce physician appointments, emergency department visits and inhospital bed days; improving the targeting of careand reducing long-term health complications.  It is not only the ill who use technology to monitor every detail of their biological processes. The  term Quantified Selfdescribes a movement in which people are exploiting wearable sensors to track,  visualize, analyze and share their states, movements and performance. Fitness products and sleep  monitors are some of the more popular self-quantification tools, and their users populate real-time  data streams and global data factories.  Which treatment works best for specific patients?  Studies have shown that wide variations exist in healthcare practices, providers, patients, outcomes  and costs across different regions. Analyzing large datasets of patient characteristics, outcomes of  treatments and their cost can help identify the mostclinically effective and cost-efficient treatments  to apply. Comparative effectiveness research has the potential to reduce incidences of ‘overtreatment’, where interventions do more harm than good, and ‘under-treatment’, where a specific  therapy should have been prescribed but was not.In the long run, over- and under-treatment both  have the potential for worse outcomes at higher costs.  Scaling-up comparative effectiveness research can change how we view global health and improve  the way public health crises are managed. Consider pneumonia, the single largest cause of child  death worldwide. According to WHO data , each year the disease claims the lives of more than 1.2  million children under the age of five – more than AIDS, malaria and tuberculosis combined.  Pneumonia is preventable with simple interventions and can be treated with low-cost, low-tech  medication and care. However, the growing resistance of the bacterium to conventional antibiotics  does underline an urgent need for vaccination campaigns to control the disease. Health data is vital  in getting this message across to policy makers, aid organizations and donors, but, no matter how  accurate and complete raw statistics and endless spreadsheets may be, their form is not one that  lends itself to easy analysis and interpretation. Models, analytics and visualizations of deep oceans of  data work together to provide a view of a particular problem in the context of other problems, as  well as in the contexts of time and geography. Data starts ‘telling its life story’, in turn becoming a  vital decision making tool. The Global Health Data Exchange is such a go-to repository for population  health data enriched by a set of tools to visualize and explore the data. Analyzing global disease patterns and identifying trendsat an early stage is mission critical for actors  in the pharmaceutical and medical products sector, allowing them to model future demand and costs  for their products and so make strategic R&D investment decisions.  High-throughput biology harnesses advances in robotics, automated digital microscopy and other lab  technologies to automate experiments in a way that makes large-scale repetition feasible. For  example, work that might once have been done by a single lab technician with a microscope and a  pipette can now be done at high speed, on a large scale. It is used to define better drug targets, i.e.,  nucleic acids or native proteins in the body whose activity can be modified by a drug to result in a  desirable therapeutic effect. Automated experiments generate very large amounts of data about disease mechanisms and they  deliver data of great importance in the early stages of drug discovery. Combined with other medical  datasets, they allow scientists to analyze biological pathways systematically, leading to an  understanding of how these pathways could be manipulated to treat disease. Data to solve the mysteries of the universe  Located just a few minutes’ drive from ITU headquarters, CERN is host to one of the biggest known  experiments in the world, as well as an example of big data, par excellence. For over 50 years, CERN  has been tackling the growing torrents of data produced by its experiments studying fundamental  particles and the forces by which they interact. The Large Hadron Collider (LHC) consists of a  27-kilometer ring of superconducting magnets with a number of accelerating structures to boost the  energy of the particles along the way. The detector sports 150 million sensors and acts as a 3D  camera, taking pictures of particle collision events at the speed of 40 million times per second. Recognizing that this data likely holds many ofthe long-sought answers to the mysteries of the  universe, and responding to the need to store, distribute and analyze the up to 30 petabytes of data  produced each year, the Worldwide LHC Computing Grid was established in 2002 to provide the  necessary global distributed network of computer centers. A lot of CERN’s data is unstructured and  only indicates thatsomething has happened. Scientists around the world now collaborate to  structure, reconstruct and analyze whathas happened and why.  Understanding the movement of people  Mobility is a major challenge for modern, growing cities, and the transport sector is innovating to  increase efficiency and sustainability. Passengers swiping their RFID-based public transport pass  leave a useful trace that helps dispatchers to analyze and direct fleet movements. Companies, road  operators and administrations possess enormous databases of vehicle movements based on GPS  probe data, sensors and traffic cameras, and they are making full use of these data treasure chests to  predict traffic jams in real time, route emergency vehicles more effectively, or, more generally,  better understand traffic patterns and solve traffic-related problems.  Drivewise.ly and Zendrive are two California-based startups working on data-driven solutions aimed  at making drivers better, safer and more eco-friendly. The assumption is that driving habits and  commuting patterns can be recognized or learned by collecting the data captured with the sensors of  a driver’s smartphone (e.g., GPS, accelerometer) and referencing it to datasets collected elsewhere.  Taken in the context of data derived from a larger community of drivers, drivers gain insights such as  “leave 10 minutes earlier to reduce your commute time by 20 minutes”, and adapting one’s driving  style can in turn help reduce fuel consumption and emissions. Data collected and analyzed by such  apps can attest you for a defensive driving style, which could help in renegotiating your insurance  premium. Your mobile phone leaves mobility traces too, and this is exploited as a resource for transport  modeling. This is of particular interest where other transport-related data is scarce. City and  transport planning was one of the themesof the ‘Data for Development’ challenge launched  by  telecommunication provider Orange in summer 2012.Participants were given access to anonymized  datasets provided by the company’s Côte d’Ivoire branch which contained 2.5 billion records of calls  and text messages exchanged between 5 million users over a period of 5 months. Situated on a lagoon with only a few bridges connecting its districts, Abidjan, the capital of Côte  d’Ivoire is experiencing major traffic congestion. As it drafts a new urban transport plan for individual  and collective means of transportation, call records offer an informative set of data on the mobility  of the population. Selecting the calls made from a residential area during the evening hours (i.e.,  when people are at home), and monitoring the locations of the calls made on the same phones  throughout the following day, produces data which reveals how many people commute, as well as  where and at what times – resulting in mobility maps which inform decisions on road and transport  investment. Box 1 details a case where Korea Telecom helped the City of Seoul determine optimal  night bus routes. Box 2 showcases a similar analysis in Geneva, Switzerland.  On a larger geographical scale, cell phone data contributes to analysis of migration patterns and is  invaluable in crisis management. Launched by the Executive Office of the United Nations SecretaryGeneral in the wake of “The Great Recession”, Global Pulse is an innovation initiative established in  response to the need for more timely information to track and monitor the impacts of global and  local socio-economic crises. The initiative is exploring how new, digital data sources and real-time  analytics technologies can help policymakers understand human well-being and emerging  vulnerabilities in real time, in the interests of better protecting populations from the aftershock of  financial and political crises. Global Pulse is a strong advocate of big data for development and  humanitarian purposes. Monetizing network data assets  Some telecommunications operators have started exploiting aggregated customer data as a source  of income by providing analytics on anonymized datasets to third parties. Long used exclusively for  network management, billing and meeting lawful intercept requirements , communications  metadata – data containing information on who sent a message, who received it, and when and  where it was sent – may represent yet another way for telecoms players to capitalize on big data  during planning, rollout, operation and upgrade phases of network infrastructure deployments.  By extracting detailed traffic information in real time, network analytics help providers to optimize  their routing network assets and to predict faults and bottlenecks before they cause any harm. Based  on customer value and behavior metrics, the customer may dynamically be offered personalized  solutions to respond to such situations. Combined real-time network insights and complete customer  profiles add value with tailor-made offerings thatincrease revenue opportunities and attract and  retain customers. Network analytics are also an important means to detect and mitigate denial of  service (DoS) attacks.  According to the vision of network equipment manufacturer Ericsson, more than 50 billion devices  will be connected by 2020, independent of what and where they are, enabling an Internet of Things  and Places. All of these devices will measure, sense, generate and communicate data of some size  and structure (e.g., a smart meter sends an energy value to the utility provider once a month; a CCTV  camera sends a 24/7 live video feed to the control room).  ITU estimates almost 7 billion mobile-cellular subscriptions worldwide (see Figure 2), and each of  these subscribers is both a data creator and consumer. Today almost 3 billion people use the Internet  and mobile-broadband subscriptions, in particular, have rocketed upwards from 268 million in 2007  to 2.1 billion in 2013. Each of these consumers contribute to the data deluge, with SMS; calls; photos,  videos and messages posted on social media sites; emails; searches; clicks on links and ads; online  shopping and mobile payments; or location traces left by GPS-enabled smartphones and WiFi  network logins.  The rapid uptake of broadband also enables the use of data-heavy services (video, video telephony,  etc.) and is reflected in the amount of data being transported over IP networks. And these figures are  quickly evolving. Proclaimed only five years ago, the exabyte era (10 bytes or over 36,000 years’  worth of HD video) was quickly superseded by the zettabyte era (10 bytes or a trillion gigabytes),  and it is safe to assume that it is just a matter of time before the dawn of the yottabyte era.  Software-defined networking (SDN) – a space currently playing host to much innovation and  standardization – involves the abstraction of lower-level network functionality through algorithms  and virtual services, and it is a promising approach to satisfying the demands of big data.  Data philanthropy and open data. Global Pulse, the UN ‘big data for development’ initiative, relies on data philanthropy where  telecommunications providers share large customer datasets. Data philanthropy requires precise and  suitable legal frameworks, ethical guidelines and trusted technology solutions to safeguard the  highest standards for the preservation of privacy – a critical challenge we will address in the  following chapter.  Governments, public-sector institutions and UN organizations can make significant contributions to  the public interest by advocating for open data and releasing their datasets by default. The Open  Knowledge Foundation defines openness in the context of open data: “A piece of data or content is  open if anyone is free to use, reuse, and redistribute it –subject only, at most, to the requirement to  attribute and/or share-alike.” The political leaders of the G8 – Canada, France, Germany, Italy, Japan, Russia, the United Kingdom  and the United States of America – have set out five strategic principles aimed at unlocking the  economic potential of open data, supporting innovation, and providing greater accountability. These  principles include an expectation that all government data will be published openly by default,  alongside principles to increase the quality, quantity and re-use of the data released from 14 highvalue areas – from education to transport, health, crime and justice. Government data portals have  emerged in all parts of the world and a number of intergovernmental organizations have responded  to calls for open data with foresight and initiative (see Table 2).  At stage 2, volume and velocity can quickly escalate into major challenges, as we have indicated with  the example of CERN. Part of the solution is cloud computing, a model for delivering seemingly  infinite computing resources on demand. Cloud computing has grown exponentially over the last five  years and its pay-per-use plans havebeen adopted by organizations ofall sizes. Cloud services give  their customers significantly more flexibility and scalability, not only in terms of data storage, but  also in how processing capacity can be scaled up or down as needed.  Distributed file systems, programming models and scalable high-performance databases are big data  core technologies exploited in stages 2 and 3 of the simplified big data value chain.  Free and open source software Hadoop, MongoDB, Spark and Storm can be counted among the most  prominent enablers. Hadoop, a free and open-source implementation of the Map/Reduce  programming model , is used to process huge datasets (petabytes of data) in a scalable manner by  commissioning parallel processing capabilities which move data subsets to distributed servers (see  Box 4). In addition, Hadoop provides a distributed file system (HDFS) that can store data on thousands of compute nodes, providing very high aggregate bandwidth across the whole cluster.  Both Map/Reduce and the distributed file system are designed so that node failures are  automatically handled by the framework to re-assign the tasks among other nodes. CERN uses MongoDB as the primary back-end for the data aggregation of one of the LHC particle  detectors. Data and metadata for the detector come from many different sources and are  distributed in a variety of digital formats. It is organized and managed by constantly evolving  software, which makes use of both relational and non-relational data sources. The back-end allows  researchers to query data via free text-based queries, then it aggregates the results from across  distributed providers and represents the data in a defined format, all the while preserving integrity,  security policy and data formats. MongoDB is a so-called ‘NoSQL’ database which refers to  mechanisms for storage and retrieval of data in less constrained consistency models than traditional  relational databases. This aids simplicity of design,scalability, control, and performance in terms of  latency and throughput.  The intelligence engine is highly application-specific and generates real intelligence that can be used  to make decisions, either by graph analysis, mathematical modeling or through some form of  simulation. In the transport sector, for instance, it would have the task of interpreting traffic data  supplied by road-side sensors and probes in order to provide traffic information usable by GPS  devices. Amazon’s patented recommendation engine , in basic terms, aggregates data about your  browsing, purchasing and reading habits and then, based on a dataset of all the other customers with  similar histories, makes extrapolations about what you would like to read next. Your favorite social  network site analyses your social graph, a mathematical structure used to model pairwise relations  between objects, to suggest friends and content to you.  Sticking to the example of the book seller, the intelligence is presented in different forms and on  various occasions, such as in notifications that “customers who bought this item also bought” or in  personalized emails sent out with tailored purchase recommendations. The last stage in the  simplified big data value chain can take many other forms. It may represent a direct user experience,  such as a warning, notification or visualization , or could result in communication with a smart  system, for example by triggering a financial transaction or by adjusting traffic signals to reflect the  actual traffic flow. The year 2013 has been a revealing year in the realm of big data analysis and visualization in mass  surveillance.  Recent events have called into question the secrecy of telecommunications and the efficacy of  security mechanisms and standards. And while these privacy and data protection concerns affect all  parts of the ICT ecosystem, they are perhaps especially relevant to big data and cloud computing.  The final section of this report outlines some of the challenges commonly associated with big data, in  parallel highlighting related opportunities within the scope of ITU’s activities.  Big data stands in stark contrast to data avoidance and data minimization, two basic principles of  data protection. Big data facilitates the tracking of people’s movements, behaviors and preferences  and, in turn, helps to predict an individual’s behavior with unprecedented accuracy, often without  the individual’s consent. For instance, electronic health records and real-time self-quantification may  constitute an enormous step forward in streamlining the prescriptions of drugs or diet and fitness  plans. However, that same data is viewedby many consumers as very sensitive.  Large sets of mobile call records, even when anonymized and stripped of all personal information,  can be used to create highly unique fingerprints ofusers, which in combination with other data such  as geo-located tweets or “check-ins” may help to reveal the individual. Communications metadata  can be useful for telecom network managementand billing, but, simply put, exploiting  communications metadata on people is a form of surveillance. It not only reveals fine-grained details  about people, but it also exposes the relationship between interacting entities. As the amount of personal data and global digital information grows, so does the number of actors  accessing and using this information. Assurances must be given that personal data will be used  appropriately, in the context of the intended uses and abiding by the relevant laws.  A closely related concern is cybersecurity. Hardly a week goes by without serious breaches of data,  where personal data often falls into the wrong hands. A range of technical solutions (e.g.,  encryption, VPNs, firewalls, threatmonitoring and auditing) can help in managing data privacy and  mitigating security risks. Threats and risks need to be reassessed in view of big data, adapting  technical solutions in response. The time is ripe to review information security policies, privacy  guidelines, and data protection acts.  Achieving the big data goals set out by business and consumers will require the interworking of  multiple systems and technologies, legacy and new.Technology integration calls for standards to  facilitate interoperability among the components ofthe big data value chain. For instance, UIMA,  OWL, PMML, RIF and XBRL are key software standards that support the interoperability of data  analytics with a model for unstructured information,ontologies for information models, predictive  models, business rules and a format for financial reporting. The standards community has launched several initiatives and working groups on big data. In 2012,  the Cloud Security Alliance established a big data working group with the aim of identifying scalable  techniques for data-centric security and privacy problems. The group’s investigation is expected to  clarify best practices for security and privacy in big data, and also to guide industry and government  in the adoption of those best practices. The U.S.National Institute of Standards and Technology  (NIST) kicked-off its big data activities with a workshop in June 2012 and a year later launched a  public working group. The NIST working group intends to support secure and effective adoption of  big data by developing consensus on definitions, taxonomies, secure reference architectures and a  technology roadmap for big data analytic techniques and technology infrastructures. ISO/IEC JTC1’s  data management and interchange standards committee (SC32) has initiated a study on nextgeneration analytics and big data. The W3C has created several community groups on different  aspects of big data. We have characterized broadband, M2M, cloud computing, advances in data management and the  rise of social media as drivers of big data – not one technology in itself, but a natural outcrop of ICT  innovation and development that cuts across its many sub-disciplines. There is rapid progress in all  these areas, and we see that the latest generations of products and solutions feature unprecedented  performance and scalability in handling, processing and making sense of data while meeting the  growing demands of Volume, Velocity, Variety and Veracity.  At present, ITU’s standardization activities address individual infrastructure requirements, noting  existing work in domains including optical transport and access networks, future network capabilities  (e.g., software-defined networks), multimedia and security. A review of this work from the angle of  data-driven applications has yet to be undertaken but could yield significant results in the big data  context.  A work item has been initiated tostudy the relationship between cloud computing and big data in  view of requirements and capabilities. The recently determined  Recommendation ITU-T X.1600,  “Security framework for cloud computing” , matches security threats with mitigation techniques, and the future standardization of the described threat-mitigation techniques is expected to  incorporate big data use cases. A previous report in the Technology Watch series advocated the use  of privacy-enhancing technologies as means to implement the ‘privacy by design’ principle, which is  of course of great interest to big data applications. ITU has been accelerating its efforts to increase interoperability in electronic health applications, in  areas such as the exchange of health data and the design of personal health systems. With  the  boom of personal and wearable ‘connected health’ and fitness products in mind, if a smart wristband  could exchange data with a smartwatch of a different make (uninhibited by vendor or manufacturer  boundaries), big data would benefit with the ability to pull together and securely integrate data  collected on different devices. The home automation sector faces a similar dilemma. When Amazon launched its marketplace for  home automation products , such as programmable thermostats, light bulbs and connected door  locks, it had to guide customers to ensure that they bought interoperable products. This concerns  communications protocols, gateways and data integration platforms alike, and care should be taken  to avoid vendor lock-ins that deny consumers  the use of the latest data-driven third party  applications to reduce energy consumption or heat the house more efficiently. After doubling the efficiency of its Emmy-winning predecessor, ITU-T H.265 is on track to become the  Web’s leading video codec. Considering multimedia’s significant share in total IP traffic, the  automatic analysis of digital image, audio and video data is an area to be monitored closely.  Multimedia content analysis allowsfor automatic and rapid extractingof events, metadata or other  meaningful information, e.g., for in-video or image search, or motion analysis. Real-time multimedia  content analysis is a challenging big data task – will the future generations of codecs be ready to  support it? The open data movement is maturing, in highly industrialized as well as emerging economies. With a  number of interoperability and policy issues at hand, ITU is in an opportune position to embrace and  advance the cause of open data in partnership with the many open data champions within and  outside its membership. From the standards angle, this could include, inter alia, the development of  requirements for data reporting and mechanisms for publication, distribution and discovery of  datasets.  ITU-T Technology Watch surveys the ICT landscape  to capture new topics for standardization activities.  Technology Watch Reports assess new technologies  with regard to existing standards inside and outside  ITU-T and their likely impact on future standardization. Previous reports in the series include: ICTs and Climate Change Ubiquitous Sensor Networks Remote Collaboration Tools NGNs and Energy Efficiency Distributed Computing: Utilities, Grids & Clouds The Future Internet Biometrics and Standards Decreasing Driver Distraction The Optical World Trends in Video Games and Gaming Digital Signage  Privacy in Cloud Computing E-health Standards and Interoperability E-learning Smart Cities Mobile Money  Spatial Standards.