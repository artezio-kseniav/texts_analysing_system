{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Texts Visualisation in numerical 3-dimensional space for articles in big data standards area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kvoronaya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kvoronaya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Term Frequency – Inverse Document Frequency (TF – IDF).\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# t-distributed Stochastic Neighbor Embedding.\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Used for text preprocessing and part of speech tagging.\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download NLTK corpuses.\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import os\n",
    "import string\n",
    "\n",
    "# Used for data representation.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Used for visualisation.\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "import pylab as pyl\n",
    "from mpl_toolkits.mplot3d import axes3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH_TEST_SET = 'C:/Users/kvoronaya/Downloads/texts_analysing_system/UPDATED_big_data_industrial/big_data_set/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** File Name Mapping *****\n",
      "0: \"before_Big-Data-New-Concerns.txt\"\n",
      "1: \"before_Big_Data_Analytics_for_Security_Intelligence.txt\"\n",
      "2: \"before_Big_Data_Taxonomy.txt\"\n",
      "3: \"before_Comment_on_Big_Data_Future_of_Privacy.txt\"\n",
      "4: \"before_CSA13-Top10Crypto.txt\"\n",
      "5: \"before_CSCC-Cloud-Customer-Architecture-for-Big-Data-and-Analytics.txt\"\n",
      "6: \"iso_ISO-IECJTC1-WG9_N0087_N0087_WD_of_ISOIEC_20546_1st_Edition.txt\"\n",
      "7: \"iso_N0147_ISO_IEC_20546_2nd_WorkingDraft.txt\"\n",
      "8: \"iso_N0200_ISO-IEC_20546_Committee_Draft.txt\"\n",
      "9: \"itu_ITU-T-A5-TD-new-Y.txt\"\n",
      "10: \"itu_ITUbroshure.txt\"\n",
      "11: \"itu_T-REC-Y3600-201511.txt\"\n",
      "12: \"nist_NISTSP1500-1.txt\"\n",
      "13: \"nist_NISTSP1500-2.txt\"\n",
      "14: \"nist_NISTSP1500-4.txt\"\n"
     ]
    }
   ],
   "source": [
    "file_name_mapping = {}\n",
    "test_set = []\n",
    "count = 0\n",
    "\n",
    "for file in os.listdir(PATH_TEST_SET):\n",
    "    if file.endswith(\".txt\"):\n",
    "        file_name_mapping[count] = file\n",
    "        count += 1\n",
    "        with open(PATH_TEST_SET + file, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.decode(\"ascii\", \"ignore\")\n",
    "                # document should always contain only one line\n",
    "                test_set.append(line)\n",
    "\n",
    "print('***** File Name Mapping *****')           \n",
    "for k, v in file_name_mapping.items():\n",
    "    print('{}: \"{}\"'.format(k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colors_mapping = {\n",
    "    0: 'crimson',\n",
    "    1: 'deeppink',\n",
    "    2: 'purple',\n",
    "    3: 'magenta',\n",
    "    4: 'palevioletred',\n",
    "    5: 'plum',\n",
    "    \n",
    "    6: 'darkgreen',\n",
    "    7: 'olive',\n",
    "    8: 'yellowgreen',\n",
    "    \n",
    "    9: 'navy',\n",
    "    10: 'blue',\n",
    "    11: 'dodgerblue',\n",
    "    \n",
    "    12: 'peru',\n",
    "    13: 'orange',\n",
    "    14: 'gold',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_preprocessing(test_set):\n",
    "    prepared_test_set = list()\n",
    "    for document in test_set:\n",
    "        \n",
    "        # tokenization – process of converting a text into tokens\n",
    "        tokens = word_tokenize(document)\n",
    "        \n",
    "        # remove stop-words\n",
    "        filtered_doc_words = [word for word in tokens if word not in stopwords.words('english')]\n",
    "        \n",
    "        # lemmatization process - procedure of obtaining the root form of the word\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        lemmatization_words = []\n",
    "        for word in filtered_doc_words:\n",
    "            lemm_word = wordnet_lemmatizer.lemmatize(word)\n",
    "            lemmatization_words.append(lemm_word)\n",
    "        \n",
    "        prepared_doc = ' '.join(lemmatization_words)\n",
    "        prepared_test_set.append(prepared_doc)\n",
    "        \n",
    "    return prepared_test_set\n",
    "\n",
    "prepared_test_set = text_preprocessing(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [i for i in tokens if i not in string.punctuation]\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ngramms_repr(ngramm_value):\n",
    "    # TF-IDF matrix for corpus, extracting 4-gramms\n",
    "    corpus_tfidf = TfidfVectorizer(\n",
    "        ngram_range=ngramm_value, stop_words='english', tokenizer=tokenize)\n",
    "    \n",
    "    corpus_representation = corpus_tfidf.fit_transform(prepared_test_set)\n",
    "    features= corpus_tfidf.get_feature_names()\n",
    "    corpus_representation_arr = corpus_representation.toarray()\n",
    "    ngramm_representation = corpus_representation_arr.transpose()\n",
    "    \n",
    "    return ngramm_representation, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize3DData(X, mappingfeatures, features, ngramm):\n",
    "    # visualize data in 3d plot with popover next to mouse position.\n",
    "\n",
    "    fig = plt.figure(figsize=(50,50))\n",
    "    ax = fig.add_subplot(111, projection = '3d')\n",
    "    set_labels = set()\n",
    "    \n",
    "    for i in xrange((int(X.shape[0])-1)):\n",
    "        xs = X[i,0]\n",
    "        ys = X[i,1]\n",
    "        zs = X[i,2]\n",
    "\n",
    "        color = None\n",
    "        label = None\n",
    "\n",
    "        if i in mappingfeatures[0]:\n",
    "            color = colors_mapping[0]\n",
    "            label = '{}'.format(file_name_mapping[0])\n",
    "\n",
    "        if i in mappingfeatures[1]:\n",
    "            color = colors_mapping[1]\n",
    "            label = '{}'.format(file_name_mapping[1])\n",
    "\n",
    "        if i in mappingfeatures[2]:\n",
    "            color = colors_mapping[2]\n",
    "            label = '{}'.format(file_name_mapping[2])\n",
    "\n",
    "        if i in mappingfeatures[3]:\n",
    "            color = colors_mapping[3]\n",
    "            label = '{}'.format(file_name_mapping[3])\n",
    "\n",
    "        if i in mappingfeatures[4]:\n",
    "            color = colors_mapping[4]\n",
    "            label = '{}'.format(file_name_mapping[4])\n",
    "\n",
    "        if i in mappingfeatures[5]:\n",
    "            color = colors_mapping[5]\n",
    "            label = '{}'.format(file_name_mapping[5])\n",
    "\n",
    "        if i in mappingfeatures[6]:\n",
    "            color = colors_mapping[6]\n",
    "            label = '{}'.format(file_name_mapping[6])\n",
    "\n",
    "        if i in mappingfeatures[7]:\n",
    "            color = colors_mapping[7]\n",
    "            label = '{}'.format(file_name_mapping[7])\n",
    "\n",
    "        if i in mappingfeatures[8]:\n",
    "            color = colors_mapping[8]\n",
    "            label = '{}'.format(file_name_mapping[8])\n",
    "\n",
    "        if i in mappingfeatures[9]:\n",
    "            color = colors_mapping[9]\n",
    "            label = '{}'.format(file_name_mapping[9])\n",
    "\n",
    "        if i in mappingfeatures[10]:\n",
    "            color = colors_mapping[10]\n",
    "            label = '{}'.format(file_name_mapping[10])\n",
    "\n",
    "        if i in mappingfeatures[11]:\n",
    "            color = colors_mapping[11]\n",
    "            label = '{}'.format(file_name_mapping[11])\n",
    "\n",
    "        if i in mappingfeatures[12]:\n",
    "            color = colors_mapping[12]\n",
    "            label = '{}'.format(file_name_mapping[12])\n",
    "\n",
    "        if i in mappingfeatures[13]:\n",
    "            color = colors_mapping[13]\n",
    "            label = '{}'.format(file_name_mapping[13])\n",
    "\n",
    "        if i in mappingfeatures[14]:\n",
    "            color = colors_mapping[14]\n",
    "            label = '{}'.format(file_name_mapping[14])\n",
    "\n",
    "        if label in set_labels:\n",
    "            label = None\n",
    "        else:    \n",
    "            set_labels.add(label)\n",
    "            \n",
    "        ax.scatter(xs, ys, zs, c=color, marker='o', alpha=0.5, s=120, label=label)\n",
    "        \n",
    "    ax.set_title('{N}-grams document visualisation in 3d space,\\n based on TF-IDF matrix \\n \\n'.format(N=ngramm))\n",
    "    ax.set_xlim(min(X[:,0]), max(X[:,0]))\n",
    "    ax.set_ylim(min(X[:,1]), max(X[:,1]))\n",
    "    ax.set_zlim(min(X[:,2]), max(X[:,2]))\n",
    "    plt.legend(loc='lower left', ncol=2, fontsize=10, bbox_to_anchor=(0, 0))\n",
    "\n",
    "\n",
    "    def distance(point, event):\n",
    "        # return distance between mouse position and given data point\n",
    "\n",
    "        assert point.shape == (3,), \"distance: point.shape is wrong: %s, must be (3,)\" % point.shape\n",
    "\n",
    "        # Project 3d data space to 2d data space\n",
    "        x2, y2, _ = proj3d.proj_transform(point[0], point[1], point[2], plt.gca().get_proj())\n",
    "        # Convert 2d data space to 2d screen space\n",
    "        x3, y3 = ax.transData.transform((x2, y2))\n",
    "\n",
    "        return np.sqrt ((x3 - event.x)**2 + (y3 - event.y)**2)\n",
    "\n",
    "\n",
    "    def calcClosestDatapoint(X, event):\n",
    "        # calculate which data point is closest to the mouse position.\n",
    "\n",
    "        distances = [distance (X[i, 0:3], event) for i in range(X.shape[0])]\n",
    "        return np.argmin(distances)\n",
    "\n",
    "\n",
    "    def annotatePlot(X, index):\n",
    "        #create popover label in 3d chart\n",
    "        # if we have previously displayed another label, remove it first\n",
    "        if hasattr(annotatePlot, 'label'):\n",
    "            annotatePlot.label.remove()\n",
    "        # Get data point from array of points X, at position index\n",
    "        x2, y2, _ = proj3d.proj_transform(X[index, 0], X[index, 1], X[index, 2], ax.get_proj())\n",
    "        annotatePlot.label = plt.annotate( '{}'.format(features[index]),\n",
    "            xy = (x2,y2), xytext = (-60, 60), textcoords = 'offset points', ha = 'right', \n",
    "            va = 'top', fontsize = 20,\n",
    "            bbox = dict(boxstyle = 'round,pad=0.8', fc = 'yellow', alpha = 1, ec=(1., .5, .5)),\n",
    "            arrowprops = dict(arrowstyle = 'fancy', connectionstyle = 'arc3,rad=0'))\n",
    "        fig.canvas.draw()\n",
    "\n",
    "\n",
    "    def onMouseMotion(event):\n",
    "        # event that is triggered when mouse is moved\n",
    "        # shows text annotation over data point closest to mouse\n",
    "        closestIndex = calcClosestDatapoint(X, event)\n",
    "        annotatePlot (X, closestIndex)\n",
    "\n",
    "    fig.canvas.mpl_connect('motion_notify_event', onMouseMotion)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_visualisation(get_ngramm_repr, features, reduced_matr, ngramm):\n",
    "    \n",
    "    ngramms_number, texts_number = get_ngramm_repr.shape\n",
    "    \n",
    "    mappingfeatures = {}\n",
    "\n",
    "    for i in xrange(int(texts_number)):\n",
    "        features_presented = []\n",
    "        corpus_representation_arr = get_ngramm_repr.transpose()\n",
    "        doc_raw = enumerate(corpus_representation_arr[i])\n",
    "        for ngram in doc_raw:\n",
    "            if ngram[1] > 0.0:\n",
    "                features_presented.append(ngram[0])\n",
    "\n",
    "        mappingfeatures[i] = features_presented\n",
    "    \n",
    "    X = reduced_matr\n",
    "    visualize3DData(X, mappingfeatures, features, ngramm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----------------------------- Extraction of 4-gramms and Visualisation -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23387L, 15L)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_4gramm_repr = get_ngramms_repr((4,4))\n",
    "\n",
    "repr_4gramms = get_4gramm_repr[0]\n",
    "features_4gramm = get_4gramm_repr[1]\n",
    "\n",
    "repr_4gramms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# similar to PCA method for dimension reduction\n",
    "newtsne = TSNE(n_components=3)\n",
    "\n",
    "reduced_matr_4gramm = newtsne.fit_transform(repr_4gramms)\n",
    "reduced_matr_4gramm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23387L, 3L)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_matr_4gramm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "make_visualisation(repr_4gramms, features_4gramm, reduced_matr_4gramm, 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
