NIST Special Publication 1500-2. NIST Big Data Interoperability  Framework:  Volume 2, Big Data Taxonomies. The Information Technology Laboratory (ITL) at NIST promotes the U.S. economy and public welfare by  providing technical leadership for the Nation’s measurement and standards infrastructure. ITL develops  tests, test methods, reference data, proof of concept implementations, and technical analyses to advance  the development and productive use of information technology. ITL’s responsibilities include the  development of management, administrative, technical, and physical standards and guidelines for the  cost-effective security and privacy of other than national security-related information in federal  information systems. This document reports on ITL’s research, guidance, and outreach efforts in  Information Technology and its collaborative activities with industry, government, and academic  organizations. Big Data is a term used to describe the large amountof data in the networked, digitized, sensor-laden,  information-driven world. While opportunities exist with Big Data, the data can overwhelm traditional  technical approaches and the growth of data is outpacing scientific and technological advances in data  analytics. To advance progress in Big Data, the NIST Big Data Public Working Group (NBD-PWG) is  working to develop consensus on important, fundamentalconcepts related to Big Data. The results are  reported in the NIST Big Data Interoperability Frameworkseries of volumes. This volume, Volume 1,  contains a definition of Big Data and related terms necessary to lay the groundwork for discussions  surrounding Big Data.  The NIST Big Data Public Working Group (NBD-PWG) Definitions and Taxonomy Subgroup prepared  this NIST Big Data Interoperability Framework: Volume 1, Definitionsto address fundamental concepts  needed to understand the new paradigm for data applications, collectively known as Big Data, and the  analytic processes collectively known as data science. While Big Data has been defined in a myriad of  ways, the shift to a Big Data paradigm occurs when the scale of the data leads to the need for a cluster of  computing and storage resources to provide cost-effective data management. Data science combines  various technologies, techniques, and theories from various fields, mostly related to computer science and  statistics, to obtain actionable knowledge from data. This report seeks to clarify the underlying concepts  of Big Data and data science to enhance communication among Big Data producers and consumers. By  defining concepts related to Big Data and data science, a common terminology can be used among Big  Data practitioners.  The NIST Big Data Interoperability Framework consists of seven volumes, each of which addresses a  specific key topic, resulting from the work of the NBD-PWG. The seven volumes are as follows:  Definitions,  Taxonomies,  Use Cases and General Requirements,  Security and Privacy, Architectures White Paper Survey, Reference Architecture,  Standards Roadmap. The NBD-PWG Definitions and Taxonomy Subgroup focused on identifying Big Data concepts, defining  terms needed to describe this new paradigm, and defining reference architecture terms. This taxonomy  provides a hierarchy of the components of the reference architecture. It is designed to meet the needs of  specific user groups, as follows:  For managers, the terms will distinguish the categorization of techniques needed to  understand this changing field.  For procurement officers, it will provide the framework for discussing organizational  needs and distinguishing among offered approaches.  For marketers, it will provide the means to promote Big Data solutions and innovations.  For the technical community, it will provide a common language to better differentiate  Big Data’s specific offerings.  This document derives from discussions in the NBD-PWG Definitions and Taxonomy Subgroup and with  interested parties. This volume provides the taxonomy of the components of the NBDRA. This taxonomy  was developed using a mind map representation, which provided a mechanism for multiple inputs and  easy editing.  It is difficult to describe the new components of Big Data systems without fully describing the context in  which they reside. The Subgroup attempted to describe only what has changed in the shift to the new Big  Data paradigm, and only the components needed to clarify this shift. For example, there is no attempt to  create a taxonomy of analytics techniques as these predate Big Data. This taxonomy will be a work in  progress to mature as new technologies are developed and the patterns within data and system  architectures are better understood.  In addition to the reference architecture taxonomy, the Subgroup began the development of a data  hierarchy.  This document provides multiple hierarchical presentations related to Big Data.  The first presentation is the taxonomy for the NBDRA. This taxonomy provides the terminology and  definitions for the components of technical systems that implement technologies for Big Data. Section 2  introduces the NBDRA using concepts of actors and roles and the activities each performs. In the  NBDRA presented in NIST Big Data Interoperability Framework Volume 6: Reference Architecture,  there are two roles that span the activities within the other roles: Management, and Security and Privacy.  These two topic areas will be addressed further in future versions of this document. The NBDRA  components are more fully described in the NIST Big Data Interoperability Framework: Volume 6,  Reference Architectureand the NIST Big Data Interoperability Framework: Volume 4, Security and  Privacydocuments. Comparing the related sections in these two documents will give the reader a more  complete picture of the consensus of the working groups.  The second presentation is a hierarchical description about the data itself. For clarity, a strict taxonomy is  not followed; rather, data is examined at different groupings to better describe what is new with Big Data.  The grouping-based description presents data elements, data records, datasets, and multiple datasets. This  examination at different groupings provides a way to easily identify the data characteristics that have  driven the development of Big Data engineering technologies, as described in the NIST Big Data  Interoperability Framework: Volume 1, Definitions.  Within the following sections, illustrative examples are given to facilitate understanding of the role/actor  and activity of the NBDRA. There is no expectation of completeness in the components; the intent is to  provide enough context to understand the specific areas that have changed because of the new Big Data  paradigm. Likewise, the data hierarchy only expresses the broad overview of data at different levels of  granularity to highlight the properties that drive the need for Big Data architectures.  For descriptions of the future of Big Data and opportunities to use Big Data technologies, the reader is  referred to the NIST Big Data Interoperability Framework: Volume 7, Standards Roadmap. Finally, to  understand how these systems are organized and integrated to meet users’ needs, the reader is referred to  NIST Big Data Interoperability Framework:Volume 3, Use Cases and General Requirements. As mentioned in the previous section, the Subgroup is continuing to explore the changes in both  Management and in Security and Privacy. As changes in the activities within these roles are clarified, the  taxonomy will be developed further.In addition, a fuller understanding of Big Data and its technologies should consider the interactions between the characteristics of the data and the desired methods in both  technique and time window for performance. These characteristics drive the application and the choice of  tools to meet system requirements. Investigation of the interfaces between data characteristics and  technologies is a continuing task for the NBD-PWG Definitions and Taxonomy Subgroup and the NBDPWG Reference Architecture Subgroup. Finally, societalimpact issues have not yet been fully explored.  There are a number of overarching issues in the implications of Big Data, such as data ownership and data  governance, which need more examination. Big Data is a rapidly evolving field, and the initial discussion  presented in this volume must be considered a work in progress.  This section focuses on a taxonomy for the NBDRA, and is intended to describe the hierarchy of actors  and roles and the activities the actors perform in those roles. There are a number of models for describing  the technologies needed for an application, such as a layer model of network, hardware, operating system,  and application. For elucidating the taxonomy, a hierarchy has been chosen to allow placing the new  technologies within the context previous technologies. As this taxonomy is not definitive, it is expected  that the taxonomy will mature as new technologies emerge and increase understanding of how to best  categorize the different methods for building data systems. In system development, actors and roles have the same relationship as in the movies. The roles are the  parts the actors play in the overall system. One actor can perform multiple roles. Likewise, a role can be  played by multiple actors, in the sense that a team of independent entitiesperhaps from independent  organizationsmay be used to satisfy end-to-end system requirements. System development actors can  represent individuals, organizations, software, or hardware. Each activity in the taxonomy can be  executed by a different actor. Examples of actors include the following:  Sensors,  Applications, Software agents, Individuals,  Organizations,  Hardware resources,  Service abstractions.  In the past, data systems tended to be hosted, developed, and deployed with the resources of only one  organization. Currently, roles may be distributed, analogous to the diversity of actors within a given  cloud-based application. Actors in Big Data systems can likewise come from multiple organizations.  Developing the reference architecture taxonomy began with a review of the NBD-PWG analyses of the  use cases and reference architecture survey provided in NIST Big Data Interoperability Framework:  Volume 3, Use Cases and General Requirementsand NIST Big Data Interoperability Framework:  Volume 5, Reference Architecture Survey, respectively. From these analyses, several commonalities  between Big Data architectures were identified and formulated into five general architecture components,  and two fabrics interwoven in the five components. These seven itemsfive main architecture components and two fabrics interwoven in themform the  foundation of the reference architecture taxonomy.  The five main components, which represent the central roles, are summarized below and discussed in this  section (Section 2).  System Orchestrator:Defines and integrates the required data application activities into an  operational vertical system. Data Provider:Introduces new data or information feeds into the Big Data system.  Big Data Application Provider:Executes a life cycle to meet security and privacy requirements  as well as System Orchestrator-defined requirements. Big Data Framework Provider:Establishes a computing frameworkin which to execute certain  transformation applications while protecting the privacy and integrity of data. And  Data Consumer:Includes end users or other systems who use the results of the Big Data  Application Provider.  The two fabrics, which are discussed separately in Sections 3 and 4, are:  Security and Privacy Fabric  Management Fabric. Figure 2 outlines potential actors for the seven items listed above. The fivecentral roles are explained in  greater detail in the following subsections.  The System Orchestrator provides the overarching requirements that the system must fulfill, including  policy, governance, architecture, resources, and business requirements, as well as monitoring or auditing  activities to ensure that the system complies with those requirements.  The System Orchestrator role includes defining and integrating the required data application activities  into an operational vertical system. The System Orchestrator role provides system requirements, highlevel design, and monitoring for the data system. Whilethe role predates Big Data systems, some related  design activities have changed within the Big Data paradigm.  Figure 3 lists the actors and activities associated with the System Orchestrator, which are further  described below.  As the business owner of the system, the System Orchestrator oversees the business context within which  the system operates, including specifying the following:  Business goals,  Targeted business action,  Data Provider contracts and service-level agreements (SLAs),  Data Consumer contracts and SLAs, Negotiation with capabilities provider, Make/buy cost analysis. A number of new business models have been created for Big Data systems, including Data as a Service  (DaaS), where a business provides the Big Data Application Provider role as a service to other actors. In  this case, the business model is to process data received from a Data Provider and provide the transformed  data to the contracted Data Consumer. The System Orchestrator establishes all policies and regulations to be followed throughout the data life  cycle, including the following:  Policy compliance requirements and monitoring, Change management process definition and requirements,  Data stewardship and ownership.  Big Data systems potentially interact with processes and data being provided by other organizations,  requiring more detailed governance and monitoring between the components of the overall system.  A number of the design activities have changed in the new paradigm. In particular, a greater choice of  data models now exists beyond the relational model. Choosing a non-relational model will depend on the  data type. Choosing the data fields that are used to decide how to distribute the data across multiple nodes  will depend on the organization’s data analysis needs,and on the ability to use those fields to distribute  the data evenly across resources.  A Data Provider makes data available to itself or to others. The actor fulfilling this role can be part of the  Big Data system, from another system, or internal or external to the organization orchestrating the system.  Once the data is within the local system, requests to retrieve the needed data will be made by the Big Data  Application Provider and routed to the Big Data Framework Provider. Data Provider actors include those  shown in Figure 4.  While the concept of a Data Provider is not new, the greater data collection and analytics capabilities have  opened up new possibilities for providing valuable data. The U.S. government’s Open Data Initiative  advocates that federal agencies which are stewards of public data also serve the role of Data Provider.  The nine possible Data Provider activities outlinedin Figure 4 are discussed further below.  The Data Provider captures data from its own sources orothers. This activity could be described as the  capture from a data producer, whether it is a sensor oran organizational process. Aspects of the data  sources activity include both online and offline sources. Among possible online sources are the following:  Web browsers,  Sensors,  Deep packet inspection devices (e.g., bridge, router, border controller),  Mobile devices. Offline sources can include the following:  Public records, Internal records.  While perhaps not theoretically different from what has been in use before, data capture from sources is  an area that is exploding in the new Big Data paradigm. New forms of sensors are now providing not only  a number of sources of data, but also data in large quantities. Smartphones and personal wearable devices  (e.g., exercise monitors, household electric meters) can all be used as sensors. In addition, technologies  such as radio frequency identification (RFID) chips are sources of data for the location of shipped items.  Collectively, all the data-producing sensors are known as the Internet of Things (IoT). The subset of  personal information devices are often referred to as “wearable tech,” with the resulting data sometimes  referred to as “digital exhaust.”  Equally important to understanding the new Big Data engineering that has emerged in the last ten years, is  the need to understand what data characteristics have driven the need for the new technologies. In Section  2 of this document, a taxonomy was presented for the NBDRA, which is described in NIST Big Data  Interoperability Framework: Volume 6, Reference Architecture. The NBDRA taxonomy has a hierarchy  of roles/actors, and activities. To understand the characteristics of data and how they have changed with  the new Big Data Paradigm, it is illustrative to look at the data characteristics at different levels of  granularity. Understanding what characteristics have changed with Big Data can best be done by  examining the data scales of data elements, of related data elements grouped into a record that represents  a specific entity or event, ofrecords collected into a dataset, and of multiple datasetsall in turn, as  shown in Figure 10. Therefore, this section does not present a strict taxonomy, breaking down each  element into parts, but provides a description of data objects at a specific granularity, attributes for those  objects, and characteristics and sub-characteristics of the attributes. The framework described will help  illuminate areas where the driving characteristics for Big Data can be understood in the context of the  characteristics of all data. Individual data elements have naturally not changed in the new Big Data paradigm. Data elements are  understood by their data type and additional contextual data, or metadata, which provides history or  additional understanding about the data. For example, inthe context of unstructured text, a data element  would refer to a single token such as a word.  Big Data and data science represent a rapidly changing field due to the recent emergence of new  technologies and rapid advancements in methods and perspectives. This document presents a taxonomy  for the NBDRA, which is presented in NIST Big Data Interoperability Framework: Volume 6, Reference  Architecture. This taxonomy is a first attempt at providing a hierarchy for categorizing the new  components and activities of Big Data systems. This initial version does not incorporate a breakdown of  either the Management or the Security and Privacy roles within the NBDRA as those areas need further  discussion within the NBD-PWG. In addition, a description of data at different scales was provided to  place concepts being ascribed to Big Data into their context. The NBD-PWG will further develop the data  characteristics and attributes in the future, in particular determining whether additional characteristics  related to data at rest or in-motion should be described. The Big Data patterns related to transactional  constraints such as ACID (Atomicity, Consistency, Isolation, Durability—a set of properties guaranteeing  reliable processing of database transactions) have not been described here, and are left to future work as  the interfaces between resources is an important area for discussion. This document constitutes a first  presentation of these descriptions, and future enhancements should provide additional understanding of  what is new in Big Data and in specific technology implementations. 