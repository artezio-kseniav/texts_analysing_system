Information Technology— Big Data— Overview and Vocabulary. ISO (the International Organization for Standardization) and IEC (the International Electrotechnical Commission) form the specialized system for worldwide standardization. National bodies that are members of ISO or IEC participate in the development of International Standards through technical committees established by the respective organization to deal with particular fields of technical activity. ISO and IEC technical committees collaborate in fields of mutual interest. Other international organizations, governmental and non-governmental, in liaison with ISO and IEC, also take part in the work. In the field of information technology, ISO and IEC have established a joint technical committee, ISO/IEC JTC 1. International Standards are drafted in accordance with the rules given in the ISO/IEC Directives, Part 2. The main task of the joint technical committee is to prepare International Standards. Draft International Standards adopted by the joint technical committee are circulated to national bodies for voting. Publication as an International Standard requires approval by at least 75 % of the national bodies casting a vote.  Attention is drawn to the possibility that some of the elements of this document may be the subject of patent rights. ISO and IEC shall not be held responsible for identifying any or all such patent rights. ISO/IEC 20546 was prepared by Joint Technical Committee ISO/IEC JTC 1, Information Technology. The Big Data paradigm is a rapidly changing field with rapidly changing technologies. This standard will provide the normative definitions and the vocabulary needed to promote improved communication and understanding of this emerging area. This international standard will provide the conceptual overview of the emerging field of Big Data, its relationship to other technical areas and standards efforts, and the concepts ascribed to big data that are not new to Big Data. This international standard is expected to be time sensitive and will need revision over time. This International Standard provides an overview of Big Data along with a set of terms and definitions. It provides a terminological foundation for Big Data-related standards. The following International Standards contain provisions which, through reference in this text, constitute provisions of this International Standard. At the time of publication, the editions indicated were valid. All Standards are subject to revision, and parties to agreements based on this International Standard are encouraged to investigate the possibility of applying the most recent edition of the Standards listed below. Members of IEC and ISO maintain registers of currently valid International Standards. For the purposes of this document, the following terms and definitions apply. The systematic performance of operations upon data. The term big data implies datasets that are extensive in volume, velocity, or variety. The term does not, however, represent data that is simply bigger than before, since this has happened on a regular basis for decades. The specific occurrence that has led to the widespread usage of the term big data is that in the mid 2000’s, extensive datasets could no longer be handled using extant data system architectures. The new big data techniques represented a shift at that time to use distributed data processing through horizontal scaling to achieve the needed performance efficiency at an affordable cost. In the evolution of data systems, there have been a number of times when the need for efficient, cost effective data analysis has forced a change in existing technologies. For example, the move to a relational model occurred when methods to reliably handle changes to structured data led in the 1980’s to the shift to relational databases that modelled relational algebra. That was a fundamental shift in data handling. The revolution in technologies referred to as big data has arisen because the relational model could no longer efficiently handle all the needs for analysis of large and often unstructured datasets. It is not just that data is bigger than before, as data has been steadily getting larger for decades. The big data revolution is instead a one-time fundamental shift in architecture towards parallelization, just as the shift to the relational model was a one-time shift. As relational databases evolved to greater efficiencies over decades, so too will big data technologies continue to evolve. Many of the conceptual underpinnings of big data have been around for years, but years since the mid 2000’s have seen an explosion in scaling technologies and their maturation and application to scaled data systems. The term big data is overloaded in common usage, and is commonly used to represent a number of related concepts, in part because several distinct system dimensions are consistently interacting with each other. To understand this revolution, the interplay of the following aspects must be considered: the characteristics of the datasets, the analysis of the datasets, the performance of the systems that handle the data, the business considerations of cost effectiveness, and the new engineering and analysis techniques for distributed data processing using horizontal scaling.   The guidance for the choice of big data architectures is driven by four big data characteristics. Volume is one of the characteristics of datasets that is most associated with big data. Volume represents the extensive amount of data available for analysis to extract valuable information. The assumption that you can extract the most value by analysing as much of the volume of data as possible was one of the primary drivers for the creation of the new scaling technologies. Velocity is the rate of flow at which the data is created, stored, analysed or visualized. Big data velocity means a large quantity of data needs to be processed in a short amount of time. Dealing with high velocity data is commonly referred to as techniques for streaming data. Variety represents the need to analyse data from a number of domains and a number of data types. The variety of data was handled through transformations or pre-analytics to extract features that would allow integration with other data. The wider range of data formats, logical models, timescales, and semantics, which is desirous to use in analytics, complicates the integration of the variety of data. Metadata is increasingly used to aid in the integration. Variability refers to changes in data rate, format/structure, semantics, and/or quality that impact the supported application, analytic, or problem. Impacts can include the need to refactor architectures, interfaces, processing/algorithms, integration/fusion, storage, applicability, or use of the data.  The big data vocabulary consists of a number of additional terms and concepts that are in common usage. Volatility in Big Data usage refers to data volatility, or the rate of change of data over time. Given the use of horizontal scaling, one new concept is a named set of records distributed across nodes of a cluster treated as a unit. Logical data models that do not follow relational algebra for the storage and manipulation of data. A non-relational database is a database that does not follow a relational model. NoSQL, which is typically translated as “no-SQL” or “not only SQL”, is the term in common usage to refer to databases that do not conform to a relational model. Processing large data sets that are distributed across nodes of a cluster requires an algorithmic change to do alter the algorithm to process the data on each node, and then return the results to process across the entire dataset. For example, MapReduce is an implementation of a scatter-gather process for data processing. Data Science is the extraction of actionable knowledge from data through a process of discovery, or hypothesis and hypothesis testing. Value represents the benefit to the organization of the actionable knowledge derived from an analytic system. This term is not new to big data, but is often ascribed to big data due to the understanding that data has potential value that was typically not considered previously.  Value arises as an output of the implementation of big data systems. Veracity refers to the completeness and accuracy of the data and relates to the data quality issues in existence for a long time. If the analytics are causal, then the quality of every data element is extremely important. If the analytics are correlations or trending over massive volume datasets, then individual bad elements could be lost in the overall counts and the trend will still be accurate. Unstructured data, such as text, image, video, and relationship data, have been increasing in both volume and prominence. While modern relational databases tend to have support for these types of data elements, their ability to directly analyse, index, and process them has tended to be both limited and accessed via non-standard SQL extensions. The need to analyse unstructured or semi-structured data has been present for many years. However, the big data paradigm shift has increased the emphasis on the value of unstructured or relationship data, and also on different engineering methods that can handle data more efficiently. Metadata is data about data or data elements, including the description of the processing history of the data. As Big Data systems are architected to perform distributed data processing including data that is external and not under the control of the big data system, the use of metadata becomes an increasingly important concept. As open data is reused for purposes far removed from its collection, it is important that metadata be associated with any data that is made available to others. The development of algorithms for the analysis of data previously did not consider the requirements of distributed data processing, the data was typically held locally. For big data, algorithms must be adapted to explicitly accommodate the particular distribution of data across nodes. Big data systems can use the coordination of nodes in a cluster to achieve scalability in the distributed data processing. The nodes can be a physical compute system leveraging software to cause the nodes to function as a unit, or it can insert a virtualized interface on top of the physical systems to realize the advantages of cloud computing. Cloud computing is a paradigm for enabling network access to a scalable and elastic pool of shareable physical or virtual resources with self-service provisioning and administration on demand. There are several key characteristics often present in for cloud computing deployments including: broad network access, measured service, multi-tenancy, on-demand self-service, rapid elasticity and scalability, and resource pooling. Cloud computing is an infrastructure model for the development of a Big Data system. To achieve the needed scalability, systems can leverage infrastructure as a service (IaaS), data platform software can to provide a platform as a service (PaaS), or an application can be provided as software as a service (SaaS). Privacy concerns are heightened now that so much data is available about individuals from the web, social media, sensors, and so forth. Privacy in a wide sense of the term is information control by an individual, which is not only to prevent data usage disadvantageous to the individual data subjects but also to facilitate data usage beneficial to them.  A typical issue in the former respect is that integration of datasets may create personally identifiable information (PII), even if none of these datasets contains PII independently. On the other hand, the major problem in the latter respect is that personal data is mostly owned by organizations, and typically not by the data subjects themselves. This makes it hard for individuals to use their data for their own sake, and for organizations to use personal data owned by other organizations, so that noone can utilize big and deep personal data. SQL (Structured Query Language) is a standard interactive and programming language designed for querying, updating, and managing data and data set in the database management system.  SQL was first published as ISO International Standard (ISO/IEC 9075) in 1987, and it has been revised to include a larger set of features as the query language for Information technology. SQL is designed for manipulating structured data, and it is also fast becoming the default language for big data analytics. SQL provides a mature and comprehensive framework for data access supporting a broad range of advanced analytical features. Analytics is a must-have functional component of data warehouse and big data. Modern SQL databases support the discovery of columns across a wide range of data set: not only relational table/views, but also XML, JSON, spatial objects, image-style objects (Binary Large Objects and Character Large Objects), and semantic objects. Big Data typically refers to distributed data-intensive processing across the nodes of a cluster. The simulation community has been developing methods for compute-intensive processing across large clusters of nodes for many years. Given that both approaches represent extrema cases for large scale computation and large scale data analysis, techniques from both will be leveraged for spectrum of capabilities needing both compute-intensive and data-intensive computation. More and more data are being created, along with computing systems capable of analysing data. Users want to leverage the amount of data available from a variety of sensors and other data generators. This provides efficient predictive analytics to manage and control networked solutions. Typical technological advances in sensors, and the deployment of IPV6 to provide Internet connectivity to sensors creates the need for a big data system that can handle high velocity streaming data from a number of sources. This is in contrast to high volume big data systems that typically run batch jobs over a relatively small number of large datasets. This difference in the characteristics of the datasets has direct implications on the architecture and methods used for data analysis. An analysis of extended data by using statistical computing is the fundamental approach for big data. Customers can develop big data analytics system by using general-purpose programming languages. On the other hand, R is a programming language and software environment for statistical computing and graphics for Statistical Computing. The R language is widely used among statisticians and data miners for developing statistical software and data analysis. R is freely available under the GNU General Public License since 1995, and is adopted onto various operating systems. R is widely used for developing statistical computing applications that analyse big data. The needs for distributed data processing have led to a number of new programming and query languages suited to the development of big data systems, as well as new processes. An example of new programming languages and frameworks is the Spark framework for analytics. New data processing languages such as Pig have also been developed. New query languages address data in NoSQL systems. New processes include scatter-gather for distributed data processing.