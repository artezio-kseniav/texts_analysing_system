Big Data Taxonomy. In this document, we propose a six-dimensional taxonomy for big data. The main objective of this taxonomy is to help decision makers navigate the myriad choices in compute and storage infrastructures as well as data analytics techniques, and security and privacy frameworks. The taxonomy has been pivoted around the nature of the data to be analyzed. The first question: What are the various domains in which big data arise? The reason for categorizing the domains in which data arise is in order to understand the infrastructural choices and requirements that need to be made for particular types of data. All “data” is not equivalent. The particular domain in which data arises will determine the types of architecture that will be required to store it, process it, and perform analytics on it. There are several ways in which we can think about this question of data domains. Latency Requirements The first way to characterize data would be according to time span in which it needs to be analyzed: Real-time (financial streams, complex event processing (CEP), intrusion detection, fraud detection), Near real-time (ad placement), Batch (retail, forensics, bioinformatics, geodata, historical data of various types). Some of the many applications that involve data arriving “in real-time” include the following:  On-line ad optimization (including real-time bidding), High frequency online trading platforms,  Security event monitoring, Financial transaction monitoring and fraud detection, Web analytics and other kinds of dashboards, Churn prediction for online games or e-commerce, Optimizing devices, industrial plants or logistics systems based on behavior and usage, Control systems related tasks; e.g., the SmartGrid, nuclear plants,  Sentiment analysis of tweets pertaining to a topic. In most of these applications, data is constantly changing. To react to certain events, it is necessary and/or practical to  consider only relevant data over a certain time frame (“page views in the last hour” or “transactions in the last  hour/day/week/month…”), instead of taking the entirety of past data into account.   In order to select the appropriate approach and big data technology solution that is best suited to a problem at hand, it  is important to understand some of the key attributes that impact this decision. In addition to latency requirements (the  time available to compute the results), these could include the following:  Event Characteristics, including input/output data rate required by the application, Event Response Complexity. Processing complexity: What is the computational complexity of the processing task for each event? Data Domain Complexity:  What is the size of the data that has to be accessed to support such processing?  As might be expected, event response complexity that is high in both the compute and data domain aspects, when  coupled with high input/output data rates and low latency requirements poses the most severe challenges on the  underlying infrastructure.  Latency is the time available between when an input event occurs and the response to that event is needed. Stated  differently, it is the time that it takes to perform the computation that leads to a decision.  There are two broad categories that we consider here: low and high latency requirements.    We will here define “low latency” applications to be those that require a response time on the order of a few  tens of milliseconds. For applications like high frequency trading and real-time bidding or on-line ad  optimization, there is an upper limit on the latency that is acceptable in the context of the application. Often,  this is on the order of 20-50  milliseconds for on-line ad optimization systems, with high frequency trading  systems being potentially even more stringent in terms of real-time response required. While the specific  latencies are a function of the application and infrastructure, and will evolve over time, applications that we  include in this category are those requiring a “real-time” response.  We define “medium to high latency” applications to be those that need a response time on the order of a  few  seconds to minutes, to potentially a few hours. For example, in the context of applications that involve user  interaction and dashboards, it is normally acceptable if the results are updated every few seconds or even every few minutes. Most forms of reporting and longer duration data analysis can tolerate latencies on the order of  several minutes, and sometimes even hours or days.  The real question here is whether the application can (or has to) react in real-time. If data comes in at 100k events per  second, but the main business action is taken by a manager who manually examines the aggregate data once every few  days to adjust some business strategy, then low latency is not a key business issue. On the other hand, if a process  control system is being driven by data from a  set of sensors, or an enterprise is deploying a new landing page for a  website and the objective is to detect any sudden drop in the number of visitors, a more immediate response is needed.  As shown in Figure 2 below, the overall latency of a computation at a  high level is comprised of communication network  latency, computation latency, and database latency. The precise budget for each of the three broad categories (network,  compute, and database) depends highly on the application. Compute-intensive applications will leave less room for  network and database operations. When considering an appropriate big data technology platform, one of the main considerations is the latency  requirement. If low latency is not required, more traditional approaches that first collect data on disk or in memory and  then perform computations on this data later will suffice. In contrast, low latency requirements generally imply that the  data must be processed as it comes in.   It is useful to think of data as being structured, unstructured or semi-structured. We provide some examples below, with  the caveat that a formal definition that precisely delineates these categories may be elusive.  Structured data is exemplified by data contained in relational databases and spreadsheets. Structured data conforms to  a database model, which is largely characterized by the various fields that data belongs to (name, address, age and so  forth), and the data type for each field (numeric, currency, alphabetic, name, date, address). The model also has a  notion of restrictions or constraints on each field (f or example, integers in a certain range), and constraints between  elements in the various fields that are used to enforce a notion of consistency (no duplicates, cannot be scheduled in  two different places at the same time, etc.) Unstructured Data (or unstructured information) refers to information that either does not have a pre-defined data  model or is not organized in a predefined manner. Unstructured information is typically text-heavy, but may also contain  data such as dates, numbers, and facts. Other examples include the “raw” (untagged) data representing photos and  graphic images, videos, streaming sensor data, web pages, PDF files, PowerPoint presentations, emails, blog entries,  wikis, and word processing documents.  Semi-structured data lies in between structured and unstructured data. It is a type of structured data, but lacks a strict  structure imposed by an underlying data model. With semi-structured data, tags or other types of markers are used to  identify certain elements within the data, but the data doesn’t have a rigid structure from which complete semantic  meaning can be easily extracted without much further processing. For example, word processing software now can  include metadata showing the author's name and the date created, while the bulk of the document contains unstructured text. (Sophisticated learning algorithms would have to mine the text to understand what the text was  about, because no model exists that classifies the text into neat categories). As an additional nuance, the text in the  document may be further tagged as including table of contents, chapters, and sections. Emails have the sender,  recipient, date, time and other fixed fields added to the unstructured data of the email message content and any  attachments. Photos or other graphics can  be tagged with keywords such as the creator, date, location and other  content-specific keywords (such as names of people in the photos), making it possible to organize and locate graphics.  XML  and other markup languages are often used to manage semi-structured data. Yet another way to characterize the domains is to look at the types of industries that generate and need to extract  information from the data.   While the Hadoop ecosystem is a popular choice for processing large datasets in parallel using commodity computing  resources, there are several other compute infrastructures to use in various domains. Figure 5 shows the taxonomy for  the various styles of processing architectures. Computing paradigms on big data currently differ at the first level of  abstraction on whether the processing will be done in batch mode, or in real-time/near real-time on streaming data  (data that is constantly coming in and needs to be processed right away). In this section, we highlight two specific  infrastructures: Hadoop for batch processing, and Spark for real-time processing. MapReduce is a  programming model and an associated implementation for processing and generating large datasets.  Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a  reduce function that merges all intermediate values associated with the same intermediate key. Many real world tasks  are expressible in this model, as shown in the paper referenced in. Programs written in this functional style are automatically parallelized and executed on a large cluster of commodity  machines. The run-time system takes care of the details of partitioning the input data, scheduling the program's execution across a set of machines, handling machine failures, and managing the required inter-machine  communication. This allows programmers without any experience with parallel and distributed systems to utilize the  resources of a large distributed system easily. Bulk synchronous parallel processing is a model proposed originally by Leslie Valiant. In this model, processors  execute independently on local data for a number of steps. They can also communicate with other processors while  computing. But they all stop to synchronize at known points in the execution; these points are called barrier  synchronization points. This method ensures that deadlock or livelock problems can be detected easily. If an  application demands “immediate” response to each event as it occurs, some form of stream processing is needed, which essentially processes the data as it comes in. The general approach is to have a little bit of code that processes  each of the events separately. In order to speed up the processing, the stream may be subdivided, and the computation  distributed across clusters. Apache Storm is a popular framework for event processing that was developed at Twitter and promulgated by Twitter  and other companies that required this paradigm of real-time processing. Other examples are Amazon’s Kinesis, or the  streaming capabilities of MapR. These frameworks take care of the scaling onto multiple cluster nodes and come with  varying degrees of support for resilience and fault tolerance, for example, through checkpointing, to make sure the  system can recover from failure. These stream processing frameworks primarily address only parallelization of the computational load; an additional  storage layer is needed to store the results in order to be able to query them. While the “current state” of the  computation is contained in the stream processing framework, there is usually no clean way to access or  query this  information, particularly from external modules. Depending on the amount of data that is being processed, this might  further entail the need for a large, high-performance storage backend. Apache Spark (discussed in more detail below), simplistically speaking, takes a hybrid approach. Events are collected in a  buffer and then processed at fixed intervals (say every few seconds) in a batch fashion. Since Spark holds the data in  memory, it can in principle process the batches fast enough to keep up with the incoming data stream.  In summary, low latency processing generally entails some form of stream processing, with associated infrastructure for  computation and storage. It is important to note that if the application requirements are extreme (for example, if submillisecond latencies are needed), then traditional software stacks may not be good enough for the task. Specialized  software stacks or components may be have to be custom built for the application. If the application context can tolerate high latency (for example, it does not require that the results be generated within  a few seconds, or even minutes), a batch-oriented computing approach can be adopted.  In the simplest example, the application can scan through log files to do what is needed. Alternatively, all of the data can  be put into a database after which the application queries this data to compute desired results. Databases used here can  be classical SQL databases, or pure storage databases such as Cassandra, or databases that can also run aggregation  jobs, such as CouchDB. Batch processing can be scaled effectively using frameworks such as Apache Hadoop (discussed in more detail below),  provided that the underlying processing can be cast into a  map-reduce paradigm. Log data can be stored in a distributed  fashion on the cluster. The application can then run queries in a parallel fashion to reduce response times.  As Hadoop has matured, a number of projects have evolved that build on top of Hadoop. One such example is  Apache Drill, a vertical database (or column-oriented database) similar to Google’s Dremel on which BigQuery is based.  Vertical databases are optimized for tasks in which you have to scan whole tables and count entries matching some  criterion. Instead of storing the data by row as in traditional databases, data is stored by column. So instead of storing  data as in a  log file, one line per entry, one takes each field of the data and stores it together, resulting in much better IO  characteristics. HP Vertica and ParStream are other vertical databases. Some projects and products have started to replace the disk underlying the database by memory (or a combination of  flash and memory) as a  storage medium, most notably SAP Hana but also GridGain and Apache Spark to get around the  disk speed limitation. These systems are still essentially batch processing in nature, although turnaround times between  queries can be reduced considerably. Another example of a high performance solution that leverages flash based  memory systems is Aerospike.   Hadoop is the open source distributed programming and storage infrastructure that grew out of Google’s seminal  MapReduce and Google file system  papers. It is based on the paradigm of “map reduce” computing, where the  input to a computational task is first “mapped” by splitting it across various worker nodes that work on subsets of the  input independently, and a “reduce” step where the answers from all of the map sub-problems are collected and combined in some manner to form the output of the overall computational task (Figure 6). The data stored can either be  in the Hadoop filesystem as unstructured data, or in a  database as structured data. Because Hadoop is designed to work  on problems whose input data is very large and cannot fit in the disk size of a single computer, the MapReduce paradigm  is designed to take computation to where the data is stored, rather than move data to where the computation occurs.  The framework is also designed to be highly fault tolerant via data replication, and to have an architecture that keeps  track of the worker nodes’ progress via polling, and reassigns tasks to other nodes if some of the nodes should fail. In  addition, the framework will automatically partition the “map” and “reduce” computations across the compute/storage  network. All of that is done automatically by the Hadoop runtime, and the developer has only to write the map and  reduce routines for the computational task at hand.   While Hadoop is good for batch processing, it is generally considered unsuitable for data streams that are nonterminating. This is because a Hadoop job assumes that all of the data exists in files on various nodes, and will start its  Map  and Reduce phases on a  fixed amount of input to generate a fixed amount of output. For streaming applications,  where there is a steady stream of data that never stops, this model is clunky and suboptimal. Adding new map tasks  dynamically to process newly arrived inputs (while potentially removing the processing of old data as any system for  processing streams of data has to work on sliding windows) creates too much overhead and too many performance  penalties. Hadoop is also not suitable for algorithms that are iterative and depend on previously computed values. This class of  algorithms includes many types of machine learning algorithms that are critical for sophisticated data analytics, such as  online learning algorithms. Hadoop is also unsuitable for algorithms that depend on a shared global state, since the entire MapReduce model  depends on independent map tasks running in parallel without needing access to a shared state that would entail severe  performance bottlenecks due to locks, semaphores, and network delays. An example of where this occurs is in MonteCarlo simulations which are used to perform inferences in probabilistic models.   Because of these issues, a new Hadoop 2.0 has been developed (Figure 8). Crucially, this framework decouples HDFS,  resource management, and MapReduce programming, and introduces a resource management layer called YARN that  takes care of the lower level resources. An application in Hadoop 2.0 can now deploy its own application-level  scheduling routines on top of the Hadoop-managed storage and compute resources. Spark is an open source cluster computing system invented at the University of California at Berkeley that is aimed at  speeding up data analytics – both in runtime and in development. To run programs faster, Spark provides primitives for  in-memory cluster computing: a job can load data into memory and query it repeatedly much more quickly than with  disk-based systems such as Hadoop MapReduce. Spark is also intended to unify the processing stack, where currently  batch processing is done using MapReduce, interactive queries using HBase, and the processing of streams for real-time  analytics using other frameworks such Twitter’s Storm. These three stacks are difficult to maintain for consistent  metrics. Also, it is difficult to perform interactive queries on streaming data. The unified Spark stack is designed to  handle these requirements efficiently and scalably. A key concept in Spark is the resilient distributed dataset (RDD), which is a collection of objects spread across a cluster  stored in RAM  or disk. Applications in Spark can load these RDDs into the memory of a cluster of nodes and let the Spark  runtime automatically handle the partitioning of the data and its locality during runtime. This enables fast iterative  processing. A stream of incoming data can be split up into a series of batches and processed as a  sequence of smallbatch jobs. The Spark architecture allows this seamless combination of streaming and batch processing in one system. To make programming faster, Spark provides clean, concise APIs in Scala, Java, and Python. Spark can be used  interactively from the Scala and Python shells to rapidly query big data sets. Spark was initially developed for two  applications where keeping data in memory helps: iterative machine learning algorithms and interactive data mining. In  both cases, Spark has been shown to run up to 100x faster than Hadoop MapReduce. Spark is also the engine behind Shark, a fully Apache Hive-compatible data warehousing system that can run 100x  faster  than Hive. While Spark is a new engine, it can access any data source supported by Hadoop. Spark fits in seamlessly with the Hadoop 2.0 ecosystem (Figure 9) as an alternative to MapReduce, while using the same  underlying infrastructure such as YARN and the HDFS. The GraphX and MLlib libraries include sophisticated graph and  machine learning algorithms that can be run in real-time. BlinkDB is a massively parallel, approximate query engine for  running interactive SQL queries that trades off query accuracy for response time, with results annotated by meaningful  error bars. BlinkDB has been demonstrated to run 200x  faster than Hive within an error of 2-10%.   Large volumes of data are coming at a much faster velocity in varieties of formats such as multimedia and text that don’t  easily fit into a column-and-row database structure. Given these factors, many solutions have been created that provide  the scale and speed developers need when they build social, analytics, gaming, financial or medical apps with large  datasets. Figure 10 shows the taxonomy for the various types of databases that are used for big data storage. In order to scale databases here to handle the volume, velocity, and variety of data, we need to scale horizontally across  multiple servers rather than scaling vertically by upgrading a single server (by adding more RAM or increasing HDD  capacity). But scaling horizontally implies a distributed architecture in which data resides in different places. This setup  leads to a  unique challenge shared by all distributed computer systems: The CAP Theorem [9]. According to the CAP  theorem, a distributed storage system must choose to sacrifice either consistency (that everyone sees the same data) or availability (that you can always read/write) while having partition tolerance (where the system continues to operate  despite arbitrary message loss or failure of part of the system). Note that partition tolerance is not an option as  partitions in a distributed system happen when nodes crash (for whatever reason) or the network drops an arbitrary  number of packets (due to switch failures or other reasons). When the inevitable “partition” happens and a distributed  system has parts that cannot communicate with each other, the question is whether a distributed system is going to  favor consistency (meaning it will respond only to queries that satisfy consistency, and not respond when consistency  cannot be guaranteed, meaning that availability is sacrificed) or whether availability will be favored (meaning all queries  are answered even if some are inconsistent). A standard metric by which databases are judged is by their ACID properties. Atomicity requires that each transaction be all or nothing. If one part of the transaction fails, the entire  transaction fails and the database state is left unchanged.  Consistency ensures that each transaction will bring the database from one valid state to another.  Isolation ensures that concurrent execution of transactions results in a system state that would be obtained if  the transactions were executed serially. Durability means that once a transaction is committed it will remain so, even in the event of power loss,  crashes, or errors.  The ACID focus on consistency is the traditional approach taken by relational databases. In order to handle the needs of  internet and cloud based models of storage, a design philosophy at the other end of the spectrum was coined by Eric  Brewer, and called BASE: Basically Available, Soft state, Eventually consistent. Most NoSQL databases are based on  BASE principles, while choosing C or A in the CAP theorem.  As discussed earlier in this section, over the past few years various types of database and NoSQL solutions have  proliferated and differentiated themselves into key-value stores, document databases, graph databases, and NewSQL. Because of the different niches addressed by these solutions, trying to evaluate the database landscape for a particular  class of problem is an important but increasingly difficult task.   Different databases use differing methods in how data is stored and accessed. Some databases, such as Couchbase and  MongoDB, are intended to run on hardware where most of the working set is cached in RAM. Other databases, such as  Aerospike, are optimized for direct writes to solid state disk (SSD), which would give them an advantage for benchmarks  that involve lot of inserts. While in-memory databases have latency and throughput advantages, there may be scaling  issues that come into play when large datasets are being handled. SSDs have higher densities and lower per-gigabyte  costs than RAM, so it should be possible to scale much larger datasets on fewer nodes for SSD databases – valuable  when talking about very large amounts of data. Scaling by adding more RAM-based machines may end up incurring a  higher recovery-from-failure cost as a  greater number of machines incur a higher rate of failures. In addition to the differing technologies used for storing data and achieving fault-tolerance, benchmarks also need to  contend with the fact that real-world usage will have all types of scenarios for the manner in which data has to be  accessed and be reliable. In one benchmark’s study, two common scenarios were examined: applications that have  strong durability needs, in which every transaction must be committed to disk and replicated in case of node failure, and  applications that are willing to relax these requirements in order to achieve the highest possible speed. Another  study examined the throughputs and latencies achievable in a variety of scenarios including read-only transactions,  reads and writes, writes only and so forth. Another study from AmpLab at UC Berkeley compared Shark with some  representative databases in three different scenarios for specific types of queries.  In general, it is difficult to make definitive statements about performance in this fast evolving landscape. Performance is  highly dependent on the query engine used, the storage architecture, and the manner in which data has been stored.  Figure 11 is meant to be suggestive of broad categories of relative performance, and should not be read as a  definitive  ordering. Figure 12 shows how the various categories of databases compare regarding the complexity of the data items  they store versus the size of the data they can handle.  Machine learning techniques allow automatic and scalable ways in which insights from large, multi-dimensional data can  be gleaned. Broadly, machine learning is the ability for computers to automatically learn patterns and make inferences  from data. These algorithms can be classified along many different axes.  Supervised Learning – This category involves all machine learning algorithms that map input data to a  given target value or class label(s). Commonly known algorithms include classification, which is the prediction of categorical labels  (classes), and regression/prediction, which is the prediction of continuous-valued variables. In these algorithms, the  learner approximates a mapping function from a feature vector to a  set of classes (in the case of classification) or value  (in the case of regression) over a large set of training data. The necessary condition for the training data is the presence  of human labels for every data point. Such labeling is not entirely feasible for various big data applications in which it is  expensive to obtain human labels for potentially millions of data points. However, a number of big data applications  exist today that have successfully utilized smaller training datasets to achieve state-of-the-art performance on large,  unlabeled real world datasets. Some of the most widely used tools in this category include the following for  classification: Neural Networks, Decision Trees, Support Vector Machines, and Naïve Bayes; and the following for  regression/prediction: Linear regression, Polynomial regression, Radial basis functions, MARS, and Multilinear  interpolation. Unsupervised Learning - This category involves all machine learning algorithms that learn the hidden structure of input  data without requiring associated human labels. Commonly known algorithms include clustering and source signal  separation. The necessary condition for the input data is the presence of representative features that can be exploited  for meaningful knowledge discovery. This technique is especially suited to big data as applications have easy access to an abundance of large unlabeled datasets that can be processed with this learning framework. Some of the most widely   used tools in this category include K-Means Clustering, Gaussian Mixture Modeling, Spectral Clustering, Hierarchical  Clustering, Principal Component Analysis and Independent Component Analysis. Reinforcement Learning - This category involves all machine learning algorithms that learn a mapping function between  observations and actions so as to maximize a reward function. The learning algorithm is typically optimized to take an  action that will yield maximum reward over a period of time. Such learning algorithms, though popular in Robotics, have  seen limited exploration in big data community. Two widely used tools in this category include Markov Decision Process  and Q-Learning. Semi-Supervised Classification - This category uses small amounts of labeled data and fuses this information with large, unlabeled datasets to approximate an appropriate learning algorithm. This class of algorithms is specifically interesting  for the big data community as the invariable presence of large unlabeled datasets is not well exploited by traditional  supervised learning algorithms. On the other hand, semi-supervised learning techniques exploit structural commonality  between labeled and unlabeled data in an efficient manner to generalize the functional mapping over large datasets. A  few subcategories of algorithms in this taxonomy include Generative Models, Graph-Based Models and Multi-View  Models. Some of the most widely used tools in this category include Active Learning, Transfer Learning and Co-Training.   While the above classification is appropriate for simple, structured datasets, complex, unstructured datasets require and  benefit from further characterization. There are six broad categories in which this data variety can be categorized, and  the machine learning algorithms mentioned in the previous section can be adapted and/or strengthened in various ways  in order to apply to these various types of unstructured datasets. Time Series Data – A vast majority of big data applications are sensitive to time, so applying scalable machine  learning algorithms to time series data becomes an important task. Time series data are sequences of values or  events obtained over repeated measurements of time, for instance, stock market data. Existing algorithms that can  successfully model time series data include Hidden Markov Models, Markov Random Fields (Spatio-Temporal  Modeling) and Conditional Random Fields. Scaling these algorithms to big data is an active research topic. Recently,  researchers have successfully scaled a traditional Dynamic time warping (DTW) algorithm to trillions of data points.  Streaming Data – Data  here is constantly arriving, for instance, from remote sensors, retail transactions, surveillance  systems, Internet traffic, and telecommunication networks. To handle this type of data requirement, machine  learning algorithms have to be applied in an online fashion. Most machine learning algorithms require batch  processing of data such as clustering that needs to look at whole data in one pass to learn meaningful clusters. Such  techniques are not scalable to streaming data where it is computationally infeasible to store past data. Recently, a  number of distributed machine learning algorithms have been proposed that approximate such offline algorithms to  streaming data, including distributed k-means and distributed SVD. A few tools that implement these algorithms  include Apache Mahout and Vowpal Wabbit. On the other hand, there are a few algorithms that address these  challenges algorithmically such as Linear SVM, Kernel SVM, and Parallel Tree Learning. Sequence Data – Sequence data consists of sequences of ordered elements or events that are recorded with or  without a concrete notion of time. The analysis of sequential data arises in many different contexts, including  retail data analysis (determining whether customers that buy one type of item are more likely to buy another type of  item), or analysis of DNA and protein sequences. Machine learning algorithms used here frequently include Hidden  Markov Models and Sequence alignment algorithms (such as BLAST for local alignment for DNA sequences).  Graph Data – Many problems are naturally modeled as graphs, including problems that require the analysis of social  networks, the analysis of the World Wide Web, the analysis of biological networks, and the analysis or synthesis of  protein structures. Almost all large-scale graph mining algorithms can be efficiently represented in matrix format  thereby necessitating large scale matrix solvers. Existing techniques that have been shown to perform at such large  scale include Collaborative Filtering, Singular Value Decomposition (SVD), and Page Rank. Two tools that aim to  address this challenge are Presto and GraphLab. Spatial Data – Spatial databases hold space-related data such as maps, medical imaging data, remote sensing data,  VLSI chip layout data and so forth. Because of spatial relationships between data, the data cannot be assumed to be  completely independent; learning algorithms can exploit this fact. Multimedia Data – This category includes images, videos, audio, and text markups. Mining algorithms for this  category will include many digital signal processing techniques for image segmentation, motion vector analysis, and  model construction.  The accuracy  of a  classifier on a given test set is the percentage of tuples that are classified correctly (these test set  tuples with labels should not have been used to train the classifier). Similarly, the accuracy of a predictor refers to how  well a given predictor can guess the value of the predicted attribute for new or previously unseen data. The error rate or  misclassification rate of a classifier is simply the remaining percentage of tuples that were not classified correctly.  For a classifier with m classes, a confusion matrix is an m x m table. An entry CM_i_j in the table indicates the number of  tuples of class i that were labeled as class j. If a classifier has good accuracy, most of the non-diagonal entries of the  table should be close to zero. From the confusion matrix, other metrics can be computed, such as precision and  sensitivity. For a predictor, the accuracy is computed by a metric, such as the root mean squared error over a test set. Accuracy can  be estimated using one or more test sets that are independent of the training set. Estimation techniques, such as crossvalidation and bootstrapping, need to be discussed. When a learner outputs a classifier that is 100%  accurate on training data but, say, 50%  accurate on test data, when it  could have been 75% accurate on both, then we say that the learner has overfit. Overfitting can be measured using bias  and variance. Bias is the learner’s tendency to learn the same wrong thing, while variance is the tendency to learn  random things irrespective of the real thing. The generalization error for a learner can be expressed as the sum of bias squared and the variance. Hence, there is a  tradeoff when minimizing the generalization error between minimizing bias and variance simultaneously. If the  generalization error is minimized by minimizing the variance, then the bias might be high and we get extreme  underfitting. If the bias is low, but the variance high, then we get overfitting.   Abstract/Summary Visualization – Often big data analytics require data to be processed at scale (e.g. billions of records,  terabytes of data) before any meaningful correlations can be discovered. Scaling existing visualization techniques at this  level becomes a non-trivial task. A new class of visualization techniques has been proposed lately that process and  abstract or summarize such large-scale data before rendering it to visualization routine. These techniques fall  under Interactive Visualization methods. Common examples of data abstraction is binning it into histograms or  presenting them as data cubes. A number of clustering algorithms have also been proposed that extend binning-based  summarization of data to novel concepts. They have the added advantage of providing a compact, reduced dimension  representation of data.  Interactive/Real-Time Visualization – A more recent class of techniques fall under interactive visualization that have to  adapt to user interactions in real-time. Such techniques necessitate that even complex visualization mechanisms take  less than a second for a real-time navigation of data by a user. These techniques are quite powerful in the sense  that they allow users to rapidly discover important insights in the data and prove or disprove different data science  theories on top of such insights. Such techniques are also crucial to industries that rely greatly on data-driven insights.  Today a number of industry software, such as Microsoft Pivot Table and Tableau, employ similar strategies for  interactive visualization.   Securing the infrastructure of big data systems involves securing distributed computations and data stores. Securing the  data itself is of paramount importance, so we have to ensure that information dissemination is privacy-preserving and  that sensitive data is protected through the use of cryptography and granular access control. Managing enormous  volumes of data necessitates scalable and distributed solutions for not only securing data stores but also enabling  efficient audits and investigations of data provenance. Finally, the streaming data that is coming in from diverse endpoints has to be checked for integrity and can be used to perform real-time analytics for security incidents to ensure the  health of the infrastructure. We will also discuss the security issues that arise for the various forms of data discussed earlier. Streaming Data – There are two complementary security problems for streaming data depending on whether the data is  public or not. For public data, confidentiality may not be an issue, but the filtering criteria applied by individual clients,  such as governments, may be classified. For private data, confidentiality may be a concern, while at the same time  suitably modified version of the data may be disclosed to achieve specific utilities, such as predictive analytics. In “Private Searching On Streaming Data”, Ostrovsky and Skeith consider the problem of private searching on  streaming data, where they efficiently implement searching for documents that satisfy a secret criterion (such as  presence or absence of a hidden combination of hidden keywords) under various cryptographic assumptions. In their  scheme, the client can send a garbled transformation of the secret criteria to the filter nodes. The filter nodes can apply  the garbled criteria to the incoming data resulting in encrypted filtered messages, which only the client can decrypt. This  effectively hides the criteria as well as the actual data that were filtered.  The continuous nature of streaming time series data presents special technical challenges in hiding the sensitive aspects  of such data. In particular, simple-minded random perturbation of data points does not provide rigorous privacy  guarantees. In “Time series compressibility and privacy”, Papadimitriou, et al. study the trade-offs between time  series compressibility and partial information hiding and the implication of this tradeoff on the introduction of  uncertainty about individual values by perturbing them.  We have given the beginning of the taxonomy of the big data landscape along six of the most important dimensions. The  six dimensions are data domains, compute infrastructure, storage architectures, analytics, visualization, security and  privacy, and data domains. Big data infrastructure and methodology continue to evolve at a fast pace, but the underlying  technologies they are based on have, in many cases, been invented many years ago. The greatly increased digitization of  human activity and machine-to-machine communications, combined with large scale inexpensive hardware, is making  practical many previously academic ideas of parallel and distributed computing, along with new tweaks necessary to  make them even more useful in real world applications.